<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-LT.html">Linear Transformations</a> » <a href="section-IVLT.html">Invertible Linear Transformations</a> »</div></div>
<div id="title"><span id="title-content">Invertible Linear Transformations</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-IVLT" acro="IVLT" titletext="Invertible Linear Transformations">
<h3 class="section">
<span class="type">Section</span> <span class="acro">IVLT</span> <span class="titletext">Invertible Linear Transformations</span>
</h3>
<div class="introduction">
<p>In this section we will conclude our introduction to linear transformations by bringing together the twin properties of injectivity and surjectivity and consider linear transformations with both of these properties.</p>

</div>
<div class="subsection" id="subsection-IVLT" acro="IVLT" titletext="Invertible Linear Transformations">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">IVLT</span> <span class="titletext">Invertible Linear Transformations</span>
</h4>
<p>One preliminary definition, and then we will have our main definition for this section.</p>
<div class="definition" id="definition-IDLT" acro="IDLT" titletext="Identity Linear Transformation">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">IDLT</span> <span class="titletext"> Identity Linear Transformation</span>
</h5>
<p>The <em class="term">identity linear transformation</em> on the vector space $W$ is defined as
\begin{equation*}
\ltdefn{I_W}{W}{W},\quad\quad \lt{I_W}{\vect{w}}=\vect{w}
\end{equation*}
</p>
</div>
<p>Informally, $I_W$ is the “do-nothing” function.  You should check that $I_W$ is really a linear transformation, as claimed, and then compute its kernel and range to see that it is both injective and surjective.  All of these facts should be straightforward to verify (<a knowl="./knowls/exercise.IVLT.T05.knowl">Exercise IVLT.T05</a>).  With this in hand we can make our main definition.</p>
<div class="definition" id="definition-IVLT" acro="IVLT" titletext="Invertible Linear Transformations">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">IVLT</span> <span class="titletext"> Invertible Linear Transformations</span>
</h5>
<p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  If there is a function $\ltdefn{S}{V}{U}$ such that
\begin{align*}
\compose{S}{T}&amp;=I_U &amp; \compose{T}{S}&amp;=I_V
\end{align*}

then $T$ is <em class="term">invertible</em>.  In this case, we call $S$ the <em class="term">inverse</em> of $T$ and write $S=\ltinverse{T}$.</p>
</div>
<p>Informally, a linear transformation $T$ is invertible if there is a companion linear transformation, $S$, which “undoes” the action of $T$.  When the two linear transformations are applied consecutively (composition), in either order, the result is to have no real effect.  It is entirely analogous to squaring a positive number and then taking its (positive) square root.</p>
<p>Here is an example of a linear transformation that is invertible.  As usual at the beginning of a section, do not be concerned with where $S$ came from, just understand how it illustrates <a class="knowl" acro="IVLT" type="Definition" title="Invertible Linear Transformations" knowl="./knowls/definition.IVLT.knowl">Definition IVLT</a>.</p>
<div class="example" id="example-AIVLT" acro="AIVLT" titletext="An invertible linear transformation"><h5 class="example">
<a knowl="./knowls/example.AIVLT.knowl"><span class="type">Example</span> <span class="acro">AIVLT</span></a> <span class="titletext">An invertible linear transformation</span>
</h5></div>
<p>It can be as instructive to study a linear transformation that is not invertible.</p>
<div class="example" id="example-ANILT" acro="ANILT" titletext="A non-invertible linear transformation"><h5 class="example">
<a knowl="./knowls/example.ANILT.knowl"><span class="type">Example</span> <span class="acro">ANILT</span></a> <span class="titletext">A non-invertible linear transformation</span>
</h5></div>
<p>In <a class="knowl" acro="ANILT" type="Example" title="A non-invertible linear transformation" knowl="./knowls/example.ANILT.knowl">Example ANILT</a> you may have noticed that $T$ is not surjective, since the matrix $A$ was not in the range of $T$.  And $T$ is not injective since there are two different input column vectors that $T$ sends to the matrix $B$.  Linear transformations $T$ that are not surjective lead to putative inverse functions $S$ that are undefined on inputs outside of the range of $T$.  Linear transformations $T$ that are not injective lead to putative inverse functions $S$ that are multiply-defined on each of their inputs.  We will formalize these ideas in <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a>.</p>
<p>But first notice in <a class="knowl" acro="IVLT" type="Definition" title="Invertible Linear Transformations" knowl="./knowls/definition.IVLT.knowl">Definition IVLT</a> that we only require the inverse (when it exists) to be a function.  When it does exist, it too is a linear transformation.</p>
<div class="theorem" id="theorem-ILTLT" acro="ILTLT" titletext="Inverse of a Linear Transformation is a Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ILTLT</span><span class="titletext"> Inverse of a Linear Transformation is a Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then the function $\ltdefn{\ltinverse{T}}{V}{U}$ is a linear transformation.</p></div>
<div class="proof"><a knowl="./knowls/proof.ILTLT.knowl">Proof</a></div>
</div>
<p>So when $T$ has an inverse, $\ltinverse{T}$ is also a linear transformation.  Furthermore, $\ltinverse{T}$ is an invertible linear transformation and <em>its</em> inverse is what you might expect.</p>
<div class="theorem" id="theorem-IILT" acro="IILT" titletext="Inverse of an Invertible Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">IILT</span><span class="titletext"> Inverse of an Invertible Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is an invertible linear transformation.  Then $\ltinverse{T}$ is an invertible linear transformation and $\ltinverse{\left(\ltinverse{T}\right)}=T$.</p></div>
<div class="proof"><a knowl="./knowls/proof.IILT.knowl">Proof</a></div>
</div>
<div class="sage" id="sage-IVLT" acro="IVLT" titletext="Invertible Linear Transformations"><h5 class="sage">
<a knowl="./knowls/sage.IVLT.knowl"><span class="type">Sage</span> <span class="acro">IVLT</span></a> <span class="titletext">Invertible Linear Transformations</span>
</h5></div>
</div>
<div class="subsection" id="subsection-IV" acro="IV" titletext="Invertibility">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">IV</span> <span class="titletext">Invertibility</span>
</h4>
<p>We now know what an inverse linear transformation is, but just which linear transformations have inverses?  Here is a theorem we have been preparing for all chapter long.</p>
<div class="theorem" id="theorem-ILTIS" acro="ILTIS" titletext="Invertible Linear Transformations are Injective and Surjective">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ILTIS</span><span class="titletext"> Invertible Linear Transformations are Injective and Surjective</span>
</h5>
<div class="statement"><p>Suppose $\ltdefn{T}{U}{V}$ is a linear transformation.  Then $T$ is invertible if and only if $T$ is injective and surjective.</p></div>
<div class="proof"><a knowl="./knowls/proof.ILTIS.knowl">Proof</a></div>
</div>
<p>When a linear transformation is both injective and surjective, the pre-image of any element of the codomain is a set of size one (a “singleton”).  This fact allowed us to <em>construct</em> the inverse linear transformation in one half of the proof of <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a> (see <a class="knowl" acro="C" type="Proof Technique" title="Constructive Proofs" knowl="./knowls/technique.C.knowl">Proof Technique C</a>) and is illustrated in the following cartoon.  This should remind you of the very general <a class="knowl" acro="KPI" type="Diagram" title="Kernel and Pre-Image" knowl="./knowls/diagram.KPI.knowl">Diagram KPI</a> which was used to illustrate <a class="knowl" acro="KPI" type="Theorem" title="Kernel and Pre-Image" knowl="./knowls/theorem.KPI.knowl">Theorem KPI</a> about pre-images, only now we have an invertible linear transformation which is therefore surjective and injective (<a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a>).  As a surjective linear transformation, there are no vectors depicted in the codomain, $V$, that have empty pre-images.  More importantly, as an injective linear transformation, the kernel is trivial (<a class="knowl" acro="KILT" type="Theorem" title="Kernel of an Injective Linear Transformation" knowl="./knowls/theorem.KILT.knowl">Theorem KILT</a>), so each pre-image is a single vector.  This makes it possible to “turn around” all the arrows to create the inverse linear transformation $\ltinverse{T}$.
<div class="diagram" id="diagram-IVLT" acro="IVLT" titletext="Invertible Linear Transformation">
<a id="diagram-IVLT"></a><object type="image/svg+xml" data="./diagrams/IVLT.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">IVLT</span> <span class="titletext">Invertible Linear Transformation</span>
</h5>
</div>
</p>
<p>Many will call an injective and surjective function a <em class="term">bijective</em> function or just a <em class="term">bijection</em>.  <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a> tells us that this is just a synonym for the term invertible (which we will use exclusively).</p>
<p>We can follow the constructive approach of the proof of <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a> to construct the inverse of a specific linear transformation, as the next example shows.</p>
<div class="example" id="example-CIVLT" acro="CIVLT" titletext="Computing the Inverse of a Linear Transformations"><h5 class="example">
<a knowl="./knowls/example.CIVLT.knowl"><span class="type">Example</span> <span class="acro">CIVLT</span></a> <span class="titletext">Computing the Inverse of a Linear Transformations</span>
</h5></div>
<p>We will make frequent use of the characterization of invertible linear transformations provided by <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a>.  The next theorem is a good example of this, and we will use it often, too.</p>
<div class="theorem" id="theorem-CIVLT" acro="CIVLT" titletext="Composition of Invertible Linear Transformations">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">CIVLT</span><span class="titletext"> Composition of Invertible Linear Transformations</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are invertible linear transformations.  Then the composition, $\ltdefn{\left(\compose{S}{T}\right)}{U}{W}$ is an invertible linear transformation.</p></div>
<div class="proof"><a knowl="./knowls/proof.CIVLT.knowl">Proof</a></div>
</div>
<p>When a composition is invertible, the inverse is easy to construct.</p>
<div class="theorem" id="theorem-ICLT" acro="ICLT" titletext="Inverse of a Composition of Linear Transformations">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ICLT</span><span class="titletext"> Inverse of a Composition of Linear Transformations</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are invertible linear transformations. Then $\compose{S}{T}$ is invertible and $\ltinverse{\left(\compose{S}{T}\right)}=\compose{\ltinverse{T}}{\ltinverse{S}}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.ICLT.knowl">Proof</a></div>
</div>
<p>Notice that this theorem not only establishes <em>what</em> the inverse of $\compose{S}{T}$ <em>is</em>, it also duplicates the conclusion of <a class="knowl" acro="CIVLT" type="Theorem" title="Composition of Invertible Linear Transformations" knowl="./knowls/theorem.CIVLT.knowl">Theorem CIVLT</a> and also establishes the invertibility of $\compose{S}{T}$.  But somehow, the proof of <a class="knowl" acro="CIVLT" type="Theorem" title="Composition of Invertible Linear Transformations" knowl="./knowls/theorem.CIVLT.knowl">Theorem CIVLT</a> is a nicer way to get this property.</p>
<p>Does <a class="knowl" acro="ICLT" type="Theorem" title="Inverse of a Composition of Linear Transformations" knowl="./knowls/theorem.ICLT.knowl">Theorem ICLT</a> remind you of the flavor of any theorem we have seen about matrices?  (Hint:  Think about getting dressed.)  Hmmmm.</p>
<div class="sage" id="sage-CIVLT" acro="CIVLT" titletext="Computing the Inverse of a Linear Transformations"><h5 class="sage">
<a knowl="./knowls/sage.CIVLT.knowl"><span class="type">Sage</span> <span class="acro">CIVLT</span></a> <span class="titletext">Computing the Inverse of a Linear Transformations</span>
</h5></div>
</div>
<div class="subsection" id="subsection-SI" acro="SI" titletext="Structure and Isomorphism">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">SI</span> <span class="titletext">Structure and Isomorphism</span>
</h4>
<p>A vector space is defined (<a class="knowl" acro="VS" type="Definition" title="Vector Space" knowl="./knowls/definition.VS.knowl">Definition VS</a>) as a set of objects (“vectors”) endowed with a definition of vector addition ($+$) and a definition of scalar multiplication (written with juxtaposition).  Many of our definitions about vector spaces involve linear combinations (<a class="knowl" acro="LC" type="Definition" title="Linear Combination" knowl="./knowls/definition.LC.knowl">Definition LC</a>), such as the span of a set (<a class="knowl" acro="SS" type="Definition" title="Span of a Set" knowl="./knowls/definition.SS.knowl">Definition SS</a>) and linear independence (<a class="knowl" acro="LI" type="Definition" title="Linear Independence" knowl="./knowls/definition.LI.knowl">Definition LI</a>).  Other definitions are built up from these ideas, such as bases (<a class="knowl" acro="B" type="Definition" title="Basis" knowl="./knowls/definition.B.knowl">Definition B</a>) and dimension (<a class="knowl" acro="D" type="Definition" title="Dimension" knowl="./knowls/definition.D.knowl">Definition D</a>).  The defining properties of a linear transformation require that a function “respect” the operations of the two vector spaces that are the domain and the codomain (<a class="knowl" acro="LT" type="Definition" title="Linear Transformation" knowl="./knowls/definition.LT.knowl">Definition LT</a>).  Finally, an invertible linear transformation is one that can be “undone” — it has a companion that reverses its effect.  In this subsection we are going to begin to roll all these ideas into one.</p>
<p>A vector space has “structure” derived from definitions of the two operations and the requirement that these operations interact in ways that satisfy the ten properties of <a class="knowl" acro="VS" type="Definition" title="Vector Space" knowl="./knowls/definition.VS.knowl">Definition VS</a>.  When two different vector spaces have an invertible linear transformation defined between them, then we can translate questions about linear combinations (spans, linear independence, bases, dimension) from the first vector space to the second.  The answers obtained in the second vector space can then be translated back, via the inverse linear transformation, and interpreted in the setting of the first vector space.  We say that these invertible linear transformations “preserve structure.”  And we say that the two vector spaces are “structurally the same.”  The precise term is “isomorphic,” from Greek meaning “of the same form.”  Let us begin to try to understand this important concept.</p>
<div class="definition" id="definition-IVS" acro="IVS" titletext="Isomorphic Vector Spaces">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">IVS</span> <span class="titletext"> Isomorphic Vector Spaces</span>
</h5>
<p>Two vector spaces $U$ and $V$ are <em class="term">isomorphic</em> if there exists an invertible linear transformation $T$ with domain $U$ and codomain $V$, $\ltdefn{T}{U}{V}$.  In this case, we write $U\isomorphic V$, and the linear transformation $T$ is known as an <em class="term">isomorphism</em> between $U$ and $V$.</p>
</div>
<p>A few comments on this definition.  First, be careful with your language (<a class="knowl" acro="L" type="Proof Technique" title="Language" knowl="./knowls/technique.L.knowl">Proof Technique L</a>).  Two vector spaces are isomorphic, or not.  It is a yes/no situation and the term only applies to a pair of vector spaces.  Any invertible linear transformation can be called an isomorphism, it is a term that applies to functions.  Second, given a pair of vector spaces there might be several different isomorphisms between the two vector spaces.  But it only takes the existence of one to call the pair isomorphic.  Third,  $U$ isomorphic to $V$, or $V$ isomorphic to $U$?  It does not matter, since the inverse linear transformation will provide the needed isomorphism in the “opposite” direction.  Being “isomorphic to” is an equivalence relation on the set of all vector spaces (see <a class="knowl" acro="SER" type="Theorem" title="Similarity is an Equivalence Relation" knowl="./knowls/theorem.SER.knowl">Theorem SER</a> for a reminder about equivalence relations).</p>
<div class="example" id="example-IVSAV" acro="IVSAV" titletext="Isomorphic vector spaces, Archetype V"><h5 class="example">
<a knowl="./knowls/example.IVSAV.knowl"><span class="type">Example</span> <span class="acro">IVSAV</span></a> <span class="titletext">Isomorphic vector spaces, Archetype V</span>
</h5></div>
<p>In <a class="knowl" acro="IVSAV" type="Example" title="Isomorphic vector spaces, Archetype V" knowl="./knowls/example.IVSAV.knowl">Example IVSAV</a> we avoided a computation in $P_3$ by a conversion of the computation to a new vector space, $M_{22}$, via an invertible linear transformation (also known as an isomorphism).  Here is a diagram meant to illustrate the more general situation of two vector spaces, $U$ and $V$, and an invertible linear transformation, $T$.  The diagram is simply about a sum of two vectors from $U$, rather than a more involved linear combination.  It should remind you of <a class="knowl" acro="DLTA" type="Diagram" title="Definition of Linear Transformation, Additive" knowl="./knowls/diagram.DLTA.knowl">Diagram DLTA</a>.
<div class="diagram" id="diagram-AIVS" acro="AIVS" titletext="Addition in Isomorphic Vector Spaces">
<a id="diagram-AIVS"></a><object type="image/svg+xml" data="./diagrams/AIVS.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">AIVS</span> <span class="titletext">Addition in Isomorphic Vector Spaces</span>
</h5>
</div>
To understand this diagram, begin in the upper-left corner, and by going straight down we can compute the sum of the two vectors using the addition for the vector space $U$.  The more circuitous alternative, in the spirit of <a class="knowl" acro="IVSAV" type="Example" title="Isomorphic vector spaces, Archetype V" knowl="./knowls/example.IVSAV.knowl">Example IVSAV</a>, is to begin in the upper-left corner and then proceed clockwise around the other three sides of the rectangle.  Notice that the vector addition is accomplished using the addition in the vector space $V$.  Then, because $T$ is a linear transformation, we can say that the result of $\lt{T}{\vect{u}_1}+\lt{T}{\vect{u}_2}$ is  equal to $\lt{T}{\vect{u}_1+\vect{u}_2}$.  Then the key feature is to recognize that applying $\ltinverse{T}$ obviously converts the second version of this result into the sum in the lower-left corner.  So there are two routes to the sum $\vect{u}_1+\vect{u}_2$, each employing an addition from a different vector space, but one is “direct” and the other is “roundabout”.  You might try designing a similar diagram for the case of scalar multiplication (see <a class="knowl" acro="DLTM" type="Diagram" title="Definition of Linear Transformation, Multiplicative" knowl="./knowls/diagram.DLTM.knowl">Diagram DLTM</a>) or for a full linear combination.</p>
<p>Checking the dimensions of two vector spaces can be a quick way to establish that they are not isomorphic.  Here is the theorem.</p>
<div class="theorem" id="theorem-IVSED" acro="IVSED" titletext="Isomorphic Vector Spaces have Equal Dimension">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">IVSED</span><span class="titletext"> Isomorphic Vector Spaces have Equal Dimension</span>
</h5>
<div class="statement"><p>Suppose $U$ and $V$ are isomorphic vector spaces.  Then $\dimension{U}=\dimension{V}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.IVSED.knowl">Proof</a></div>
</div>
<p>The contrapositive of <a class="knowl" acro="IVSED" type="Theorem" title="Isomorphic Vector Spaces have Equal Dimension" knowl="./knowls/theorem.IVSED.knowl">Theorem IVSED</a> says that if $U$ and $V$ have different dimensions, then they are not isomorphic.  Dimension is the simplest “structural” characteristic that will allow you to distinguish non-isomorphic vector spaces.  For example $P_6$ is not isomorphic to $M_{34}$ since their dimensions (7 and 12, respectively) are not equal.  With tools developed in <a href="section-VR.html" title="Vector Representations">Section VR</a> we will be able to establish that the converse of <a class="knowl" acro="IVSED" type="Theorem" title="Isomorphic Vector Spaces have Equal Dimension" knowl="./knowls/theorem.IVSED.knowl">Theorem IVSED</a> is true.  Think about that one for a moment.</p>
</div>
<div class="subsection" id="subsection-RNLT" acro="RNLT" titletext="Rank and Nullity of a Linear Transformation">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">RNLT</span> <span class="titletext">Rank and Nullity of a Linear Transformation</span>
</h4>
<p>Just as a matrix has a rank and a nullity, so too do linear transformations.  And just like the rank and nullity of a matrix are related (they sum to the number of columns, <a class="knowl" acro="RPNC" type="Theorem" title="Rank Plus Nullity is Columns" knowl="./knowls/theorem.RPNC.knowl">Theorem RPNC</a>) the rank and nullity of a linear transformation are related.  Here are the definitions and theorems, see the Archetypes (<a href="archetypes.html" title="Archetypes">Archetypes</a>) for loads of examples.</p>
<div class="definition" id="definition-ROLT" acro="ROLT" titletext="Rank Of a Linear Transformation">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">ROLT</span> <span class="titletext"> Rank Of a Linear Transformation</span>
</h5>
<p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the <em class="term">rank</em> of $T$, $\rank{T}$, is the dimension of the range of $T$,
\begin{equation*}
\rank{T}=\dimension{\rng{T}}
\end{equation*}</p>
</div>
<div class="definition" id="definition-NOLT" acro="NOLT" titletext="Nullity Of a Linear Transformation">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">NOLT</span> <span class="titletext"> Nullity Of a Linear Transformation</span>
</h5>
<p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the <em class="term">nullity</em> of $T$, $\nullity{T}$, is the dimension of the kernel of $T$,
\begin{equation*}
\nullity{T}=\dimension{\krn{T}}
\end{equation*}</p>
</div>
<p>Here are two quick theorems.</p>
<div class="theorem" id="theorem-ROSLT" acro="ROSLT" titletext="Rank Of a Surjective Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ROSLT</span><span class="titletext"> Rank Of a Surjective Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the rank of $T$ is the dimension of $V$, $\rank{T}=\dimension{V}$, if and only if $T$ is surjective.</p></div>
<div class="proof"><a knowl="./knowls/proof.ROSLT.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-NOILT" acro="NOILT" titletext="Nullity Of an Injective Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NOILT</span><span class="titletext"> Nullity Of an Injective Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then the nullity of $T$ is zero, $\nullity{T}=0$, if and only if $T$ is injective.</p></div>
<div class="proof"><a knowl="./knowls/proof.NOILT.knowl">Proof</a></div>
</div>
<p>Just as injectivity and surjectivity come together in invertible linear transformations, there is a clear relationship between rank and nullity of a linear transformation.  If one is big, the other is small.</p>
<div class="theorem" id="theorem-RPNDD" acro="RPNDD" titletext="Rank Plus Nullity is Domain Dimension">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">RPNDD</span><span class="titletext"> Rank Plus Nullity is Domain Dimension</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation.  Then
\begin{equation*}
\rank{T}+\nullity{T}=\dimension{U}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.RPNDD.knowl">Proof</a></div>
</div>
<p><a class="knowl" acro="RPNC" type="Theorem" title="Rank Plus Nullity is Columns" knowl="./knowls/theorem.RPNC.knowl">Theorem RPNC</a> said that the rank and nullity of a matrix sum to the number of columns of the matrix.  This result is now an easy consequence of <a class="knowl" acro="RPNDD" type="Theorem" title="Rank Plus Nullity is Domain Dimension" knowl="./knowls/theorem.RPNDD.knowl">Theorem RPNDD</a> when we consider the linear transformation $\ltdefn{T}{\complex{n}}{\complex{m}}$ defined with the $m\times n$ matrix $A$ by $\lt{T}{\vect{x}}=A\vect{x}$.  The range and kernel of $T$ are identical to the column space and null space of the matrix $A$ (<a knowl="./knowls/exercise.ILT.T20.knowl">Exercise ILT.T20</a>, <a knowl="./knowls/exercise.SLT.T20.knowl">Exercise SLT.T20</a>), so the rank and nullity of the matrix $A$ are identical to the rank and nullity of the linear transformation $T$.  The dimension of the domain of $T$ is the dimension of $\complex{n}$, exactly the number of columns for the matrix $A$.</p>
<p>This theorem can be especially useful in determining basic properties of linear transformations.  For example, suppose that $\ltdefn{T}{\complex{6}}{\complex{6}}$ is a linear transformation and you are able to quickly establish that the kernel is trivial.  Then $\nullity{T}=0$.  First this means that $T$ is injective by <a class="knowl" acro="NOILT" type="Theorem" title="Nullity Of an Injective Linear Transformation" knowl="./knowls/theorem.NOILT.knowl">Theorem NOILT</a>.  Also, <a class="knowl" acro="RPNDD" type="Theorem" title="Rank Plus Nullity is Domain Dimension" knowl="./knowls/theorem.RPNDD.knowl">Theorem RPNDD</a> becomes
\begin{equation*}
6=\dimension{\complex{6}}=\rank{T}+\nullity{T}=\rank{T}+0=\rank{T}
\end{equation*}
So the rank of $T$ is equal to the dimension of the codomain, and by <a class="knowl" acro="ROSLT" type="Theorem" title="Rank Of a Surjective Linear Transformation" knowl="./knowls/theorem.ROSLT.knowl">Theorem ROSLT</a> we know $T$ is surjective.  Finally, we know $T$ is invertible by <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a>.  So from the determination that the kernel is trivial, and consideration of various dimensions, the theorems of this section allow us to conclude the existence of an inverse linear transformation for $T$.
Similarly, <a class="knowl" acro="RPNDD" type="Theorem" title="Rank Plus Nullity is Domain Dimension" knowl="./knowls/theorem.RPNDD.knowl">Theorem RPNDD</a> can be used to provide alternative proofs for <a class="knowl" acro="ILTD" type="Theorem" title="Injective Linear Transformations and Dimension" knowl="./knowls/theorem.ILTD.knowl">Theorem ILTD</a>, <a class="knowl" acro="SLTD" type="Theorem" title="Surjective Linear Transformations and Dimension" knowl="./knowls/theorem.SLTD.knowl">Theorem SLTD</a> and <a class="knowl" acro="IVSED" type="Theorem" title="Isomorphic Vector Spaces have Equal Dimension" knowl="./knowls/theorem.IVSED.knowl">Theorem IVSED</a>.  It would be an interesting exercise to construct these proofs.</p>
<p>It would be instructive to study the archetypes that are linear transformations and see how many of their properties can be deduced just from considering only the dimensions of the domain and codomain.  Then add in just knowledge of either the nullity or rank, and see how much more you can learn about the linear transformation.  The table preceding all of the archetypes (<a href="archetypes.html" title="Archetypes">Archetypes</a>) could be a good place to start this analysis.</p>
<div class="sage" id="sage-LTOE" acro="LTOE" titletext="Linear Transformation Odds and Ends"><h5 class="sage">
<a knowl="./knowls/sage.LTOE.knowl"><span class="type">Sage</span> <span class="acro">LTOE</span></a> <span class="titletext">Linear Transformation Odds and Ends</span>
</h5></div>
</div>
<div class="subsection" id="subsection-SLELT" acro="SLELT" titletext="Systems of Linear Equations and Linear Transformations">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">SLELT</span> <span class="titletext">Systems of Linear Equations and Linear Transformations</span>
</h4>
<p>This subsection does not really belong in this section, or any other section, for that matter.  It is just the right time to have a discussion about the connections between the central topic of linear algebra, linear transformations, and our motivating topic from <a href="chapter-SLE.html" title="Systems of Linear Equations">Chapter SLE</a>, systems of linear equations.  We will discuss several theorems we have seen already, but we will also make some forward-looking statements that will be justified in <a href="chapter-R.html" title="Representations">Chapter R</a>.</p>
<p><a knowl="./knowls/archetype.D.knowl">Archetype D</a> and <a knowl="./knowls/archetype.E.knowl">Archetype E</a> are ideal examples to illustrate connections with linear transformations.  Both have the same coefficient matrix,
\begin{equation*}
D=\begin{bmatrix}
2 &amp; 1 &amp; 7 &amp; -7\\
-3 &amp; 4 &amp;  -5 &amp; -6\\
1 &amp; 1 &amp; 4 &amp;  -5
\end{bmatrix}
\end{equation*}
</p>
<p>To apply the <em>theory</em> of linear transformations to these two archetypes, employ the matrix-vector product (<a class="knowl" acro="MVP" type="Definition" title="Matrix-Vector Product" knowl="./knowls/definition.MVP.knowl">Definition MVP</a>) and define the linear transformation,
\begin{equation*}
\ltdefn{T}{\complex{4}}{\complex{3}},\quad \lt{T}{\vect{x}}=D\vect{x}
=x_1\colvector{2\\-3\\1}+
x_2\colvector{1\\4\\1}+
x_3\colvector{7\\-5\\4}+
x_4\colvector{-7\\-6\\-5}
\end{equation*}
</p>
<p><a class="knowl" acro="MBLT" type="Theorem" title="Matrices Build Linear Transformations" knowl="./knowls/theorem.MBLT.knowl">Theorem MBLT</a> tells us that $T$ is indeed a linear transformation.  <a knowl="./knowls/archetype.D.knowl">Archetype D</a> asks for solutions to $\linearsystem{D}{\vect{b}}$, where $\vect{b}=\colvector{8\\-12\\-4}$.  In the language of linear transformations this is equivalent to asking for $\preimage{T}{\vect{b}}$.  In the language of vectors and matrices it asks for a linear combination of the four columns of $D$ that will equal $\vect{b}$.   One solution listed is $\vect{w}=\colvector{7\\8\\1\\3}$.  With a nonempty preimage, <a class="knowl" acro="KPI" type="Theorem" title="Kernel and Pre-Image" knowl="./knowls/theorem.KPI.knowl">Theorem KPI</a> tells us that the complete solution set of the linear system is the preimage of $\vect{b}$,
\begin{equation*}
\vect{w}+\krn{T}=\setparts{\vect{w}+\vect{z}}{\vect{z}\in\krn{T}}
\end{equation*}
</p>
<p>The kernel of the linear transformation $T$ is exactly the null space of the matrix $D$ (see <a knowl="./knowls/exercise.ILT.T20.knowl">Exercise ILT.T20</a>),  so this approach to the solution set should be reminiscent of <a class="knowl" acro="PSPHS" type="Theorem" title="Particular Solution Plus Homogeneous Solutions" knowl="./knowls/theorem.PSPHS.knowl">Theorem PSPHS</a>.  The kernel of the linear transformation is the preimage of the zero vector, exactly equal to the solution set of the homogeneous system $\homosystem{D}$.  Since $D$ has a null space of dimension two, every preimage (and in particular the preimage of $\vect{b}$) is as “big” as a subspace of dimension two (but is not a subspace).</p>
<p><a knowl="./knowls/archetype.E.knowl">Archetype E</a> is identical to <a knowl="./knowls/archetype.D.knowl">Archetype D</a> but with a different vector of constants, $\vect{d}=\colvector{2\\3\\2}$.  We can use the same linear transformation $T$ to discuss this system of equations since the coefficient matrix is identical.  Now the set of solutions to $\linearsystem{D}{\vect{d}}$  is the pre-image of $\vect{d}$, $\preimage{T}{\vect{d}}$.  However, the vector $\vect{d}$ is not in the range of the linear transformation (nor is it in the column space of the matrix, since these two sets are equal by <a knowl="./knowls/exercise.SLT.T20.knowl">Exercise SLT.T20</a>).  So the empty pre-image is equivalent to the inconsistency of the linear system.</p>
<p>These two archetypes each have three equations in four variables, so either the resulting linear systems are inconsistent, or they are consistent and application of <a class="knowl" acro="CMVEI" type="Theorem" title="Consistent, More Variables than Equations, Infinite solutions" knowl="./knowls/theorem.CMVEI.knowl">Theorem CMVEI</a> tells us that the system has infinitely many solutions.  Considering these same parameters for the linear transformation, the dimension of the domain, $\complex{4}$, is four, while the codomain, $\complex{3}$, has dimension three.  Then
\begin{align*}
\nullity{T}&amp;=\dimension{\complex{4}}-\rank{T}&amp;&amp;\knowl{./knowls/theorem.RPNDD.knowl}{\text{Theorem RPNDD}}\\
&amp;=4-\dimension{\rng{T}}&amp;&amp;\knowl{./knowls/definition.ROLT.knowl}{\text{Definition ROLT}}\\
&amp;\geq 4-3&amp;&amp;\text{$\rng{T}$ subspace of $\complex{3}$}\\
&amp;=1
\end{align*}

</p>
<p>So the kernel of $T$ is nontrivial simply by considering the dimensions of the domain (number of variables) and the codomain (number of equations).  Pre-images of elements of the codomain that are not in the range of $T$ are empty (inconsistent systems).  For elements of the codomain that are in the range of $T$ (consistent systems), <a class="knowl" acro="KPI" type="Theorem" title="Kernel and Pre-Image" knowl="./knowls/theorem.KPI.knowl">Theorem KPI</a> tells us that the pre-images are built from the kernel, and with a nontrivial kernel, these pre-images are infinite (infinitely many solutions).</p>
<p>When do systems of equations have unique solutions?  Consider the system of linear equations $\linearsystem{C}{\vect{f}}$ and the linear transformation $\lt{S}{\vect{x}}=C\vect{x}$.  If $S$ has a trivial kernel, then pre-images will either be empty or be finite sets with single elements.  Correspondingly, the coefficient matrix $C$ will have a trivial null space and solution sets will either be empty (inconsistent) or contain a single solution (unique solution).  Should the matrix be square and have a trivial null space then we recognize the matrix as being nonsingular.  A square matrix means that the corresponding linear transformation, $T$, has equal-sized domain and codomain.  With a nullity of zero, $T$ is injective, and also <a class="knowl" acro="RPNDD" type="Theorem" title="Rank Plus Nullity is Domain Dimension" knowl="./knowls/theorem.RPNDD.knowl">Theorem RPNDD</a> tells us that rank of $T$ is equal to the dimension of the domain, which in turn is equal to the dimension of the codomain.  In other words, $T$ is surjective.  Injective and surjective, and <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a> tells us that $T$ is invertible.  Just as we can use the inverse of the coefficient matrix to find the unique solution of any linear system with a nonsingular coefficient matrix (<a class="knowl" acro="SNCM" type="Theorem" title="Solution with Nonsingular Coefficient Matrix" knowl="./knowls/theorem.SNCM.knowl">Theorem SNCM</a>), we can use the inverse of the linear transformation to construct the unique element of any pre-image (proof of <a class="knowl" acro="ILTIS" type="Theorem" title="Invertible Linear Transformations are Injective and Surjective" knowl="./knowls/theorem.ILTIS.knowl">Theorem ILTIS</a>).</p>
<p>The executive summary of this discussion is that to every coefficient matrix of a system of linear equations we can associate a natural linear transformation.  Solution sets for systems with this coefficient matrix are preimages of elements of the codomain of the linear transformation.  For every theorem about systems of linear equations there is an analogue about linear transformations.  The theory of linear transformations provides all the tools to recreate the theory of solutions to linear systems of equations.</p>
<p>We will continue this adventure in <a href="chapter-R.html" title="Representations">Chapter R</a>.</p>
<div class="sage" id="sage-SUTH1" acro="SUTH1" titletext="Sage Under The Hood, Round 1"><h5 class="sage">
<a knowl="./knowls/sage.SUTH1.knowl"><span class="type">Sage</span> <span class="acro">SUTH1</span></a> <span class="titletext">Sage Under The Hood, Round 1</span>
</h5></div>
</div>
<div class="readingquestions" id="readingquestions-IVLT" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.IVLT.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-IVLT" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.IVLT.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
