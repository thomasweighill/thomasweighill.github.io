<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-M.html">Matrices</a> » <a href="section-FS.html">Four Subsets</a> »</div></div>
<div id="title"><span id="title-content">Four Subsets</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-FS" acro="FS" titletext="Four Subsets">
<h3 class="section">
<span class="type">Section</span> <span class="acro">FS</span> <span class="titletext">Four Subsets</span>
</h3>
<div class="introduction">
<p>There are four natural subsets associated with a matrix.  We have met three already: the null space, the column space and the row space.  In this section we will introduce a fourth, the left null space.  The objective of this section is to describe one procedure that will allow us to find linearly independent sets that span each of these four sets of column vectors.  Along the way, we will make a connection with the inverse of a matrix, so <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> will tie together most all of this chapter (and the entire course so far).</p>

</div>
<div class="subsection" id="subsection-LNS" acro="LNS" titletext="Left Null Space">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">LNS</span> <span class="titletext">Left Null Space</span>
</h4>
<div class="definition" id="definition-LNS" acro="LNS" titletext="Left Null Space">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">LNS</span> <span class="titletext"> Left Null Space</span>
</h5>
<p>Suppose $A$ is an $m\times n$ matrix.  Then the <em class="term">left null space</em> is defined as $\lns{A}=\nsp{\transpose{A}}\subseteq\complex{m}$.</p>
</div>
<p>The left null space will not feature prominently in the sequel, but we can explain its name and connect it to row operations.   Suppose $\vect{y}\in\lns{A}$.  Then by <a class="knowl" acro="LNS" type="Definition" title="Left Null Space" knowl="./knowls/definition.LNS.knowl">Definition LNS</a>, $\transpose{A}\vect{y}=\zerovector$.  We can then write
\begin{align*}
\transpose{\zerovector}
&amp;=\transpose{\left(\transpose{A}\vect{y}\right)}
&amp;&amp;\knowl{./knowls/definition.LNS.knowl}{\text{Definition LNS}}\\
&amp;=\transpose{\vect{y}}\transpose{\left(\transpose{A}\right)}
&amp;&amp;\knowl{./knowls/theorem.MMT.knowl}{\text{Theorem MMT}}\\
&amp;=\transpose{\vect{y}}A
&amp;&amp;\knowl{./knowls/theorem.TT.knowl}{\text{Theorem TT}}
\end{align*}

</p>
<p>The product $\transpose{\vect{y}}A$ can be viewed as the components of $\vect{y}$ acting as the scalars in a linear combination of the <em>rows</em> of $A$.  And the result is a “row vector”, $\transpose{\zerovector}$ that is totally zeros.  When we apply a sequence of row operations to a matrix, each row of the resulting matrix is some linear combination of the rows.  These observations tell us that the vectors in the left null space are scalars that record a sequence of row operations that result in a row of zeros in the row-reduced version of the matrix.  We will see this idea more explicitly in the course of proving <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a>.</p>
<div class="example" id="example-LNS" acro="LNS" titletext="Left null space"><h5 class="example">
<a knowl="./knowls/example.LNS.knowl"><span class="type">Example</span> <span class="acro">LNS</span></a> <span class="titletext">Left null space</span>
</h5></div>
<div class="sage" id="sage-LNS" acro="LNS" titletext="Left Null Spaces"><h5 class="sage">
<a knowl="./knowls/sage.LNS.knowl"><span class="type">Sage</span> <span class="acro">LNS</span></a> <span class="titletext">Left Null Spaces</span>
</h5></div>
</div>
<div class="subsection" id="subsection-CCS" acro="CCS" titletext="Computing Column Spaces">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">CCS</span> <span class="titletext">Computing Column Spaces</span>
</h4>
<p>We have three ways to build the column space of a matrix.  First, we can use just the definition, <a class="knowl" acro="CSM" type="Definition" title="Column Space of a Matrix" knowl="./knowls/definition.CSM.knowl">Definition CSM</a>, and express the column space as a span of the columns of the matrix.  A second approach gives us the column space as the span of <em>some</em> of the columns of the matrix, and additionally, this set is linearly independent (<a class="knowl" acro="BCS" type="Theorem" title="Basis of the Column Space" knowl="./knowls/theorem.BCS.knowl">Theorem BCS</a>).  Finally, we can transpose the matrix, row-reduce the transpose, kick out zero rows, and write the remaining rows as column vectors.  <a class="knowl" acro="CSRST" type="Theorem" title="Column Space, Row Space, Transpose" knowl="./knowls/theorem.CSRST.knowl">Theorem CSRST</a> and <a class="knowl" acro="BRS" type="Theorem" title="Basis for the Row Space" knowl="./knowls/theorem.BRS.knowl">Theorem BRS</a> tell us that the resulting vectors are linearly independent and their span is the column space of the original matrix.</p>
<p>We will now demonstrate a fourth method by way of a rather complicated example.  Study this example carefully, but realize that its main purpose is to motivate a theorem that simplifies much of the apparent complexity.  So other than an instructive exercise or two, the procedure we are about to describe will not be a usual approach to computing a column space.</p>
<div class="example" id="example-CSANS" acro="CSANS" titletext="Column space as null space"><h5 class="example">
<a knowl="./knowls/example.CSANS.knowl"><span class="type">Example</span> <span class="acro">CSANS</span></a> <span class="titletext">Column space as null space</span>
</h5></div>
<p>This example motivates the remainder of this section, so it is worth careful study.  You might attempt to mimic the second approach with the coefficient matrices of <a knowl="./knowls/archetype.I.knowl">Archetype I</a> and <a knowl="./knowls/archetype.J.knowl">Archetype J</a>.  We will see shortly that the matrix $L$ contains more information about $A$ than just the column space.</p>
<div class="sage" id="sage-RRSM" acro="RRSM" titletext="Row-Reducing a Symbolic Matrix"><h5 class="sage">
<a knowl="./knowls/sage.RRSM.knowl"><span class="type">Sage</span> <span class="acro">RRSM</span></a> <span class="titletext">Row-Reducing a Symbolic Matrix</span>
</h5></div>
</div>
<div class="subsection" id="subsection-EEF" acro="EEF" titletext="Extended Echelon Form">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">EEF</span> <span class="titletext">Extended Echelon Form</span>
</h4>
<p>The final matrix that we row-reduced in <a class="knowl" acro="CSANS" type="Example" title="Column space as null space" knowl="./knowls/example.CSANS.knowl">Example CSANS</a> should look familiar in most respects to the procedure we used to compute the inverse of a nonsingular matrix, <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a>.  We will now generalize that procedure to matrices that are not necessarily nonsingular, or even square.  First a definition.</p>
<div class="definition" id="definition-EEF" acro="EEF" titletext="Extended Echelon Form">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">EEF</span> <span class="titletext"> Extended Echelon Form</span>
</h5>
<p>Suppose $A$ is an $m\times n$ matrix.  Extend $A$ on its right side with the addition of an $m\times m$ identity matrix to form an $m\times (n + m)$ matrix $M$.  Use row operations to bring $M$ to reduced row-echelon form and call the result $N$.  $N$ is the <em class="term">extended reduced row-echelon form</em> of $A$, and we will standardize on names for five submatrices ($B$, $C$, $J$, $K$, $L$) of $N$.</p>
<p>Let $B$ denote the $m\times n$ matrix formed from the first $n$ columns of $N$ and let $J$ denote the $m\times m$ matrix formed from the last $m$ columns of $N$.  Suppose that $B$ has $r$ nonzero rows.  Further partition $N$ by letting $C$ denote the $r\times n$ matrix formed from all of the nonzero rows of $B$.  Let $K$ be the $r\times m$ matrix formed from the first $r$ rows of $J$, while $L$ will be the $(m-r)\times m$ matrix formed from the bottom $m-r$ rows of $J$.  Pictorially,
\begin{equation*}
M=[A\vert I_m]
\rref
N=[B\vert J]
=
\left[\begin{array}{c|c}C&amp;K\\\hline0&amp;L\end{array}\right]
\end{equation*}
</p>
</div>
<div class="example" id="example-SEEF" acro="SEEF" titletext="Submatrices of extended echelon form"><h5 class="example">
<a knowl="./knowls/example.SEEF.knowl"><span class="type">Example</span> <span class="acro">SEEF</span></a> <span class="titletext">Submatrices of extended echelon form</span>
</h5></div>
<div class="theorem" id="theorem-PEEF" acro="PEEF" titletext="Properties of Extended Echelon Form">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">PEEF</span><span class="titletext"> Properties of Extended Echelon Form</span>
</h5>
<div class="statement"><p>Suppose that $A$ is an $m\times n$ matrix and that $N$ is its extended echelon form.  Then
<ol>
<li> $J$ is nonsingular.
</li>
<li> $B=JA$.
</li>
<li> If $\vect{x}\in\complex{n}$ and $\vect{y}\in\complex{m}$, then $A\vect{x}=\vect{y}$ if and only if $B\vect{x}=J\vect{y}$.
</li>
<li> $C$ is in reduced row-echelon form, has no zero rows and has $r$ pivot columns.
</li>
<li> $L$ is in reduced row-echelon form, has no zero rows and has $m-r$ pivot columns.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.PEEF.knowl">Proof</a></div>
</div>
<p>Notice that in the case where $A$ is a nonsingular matrix we know that the reduced row-echelon form of $A$ is the identity matrix (<a class="knowl" acro="NMRRI" type="Theorem" title="Nonsingular Matrices Row Reduce to the Identity matrix" knowl="./knowls/theorem.NMRRI.knowl">Theorem NMRRI</a>), so $B=I_n$.  Then the second conclusion above says $JA=B=I_n$, so $J$ is the inverse of $A$.  Thus this theorem generalizes <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a>, though the result is a “left-inverse” of $A$ rather than a “right-inverse.”</p>
<p>The third conclusion of <a class="knowl" acro="PEEF" type="Theorem" title="Properties of Extended Echelon Form" knowl="./knowls/theorem.PEEF.knowl">Theorem PEEF</a> is the most telling.  It says that $\vect{x}$ is a solution to the linear system $\linearsystem{A}{\vect{y}}$ if and only if $\vect{x}$ is a solution to the linear system $\linearsystem{B}{J\vect{y}}$.  Or said differently, if we row-reduce the augmented matrix $\augmented{A}{\vect{y}}$ we will get the augmented matrix $\augmented{B}{J\vect{y}}$.  The matrix $J$ tracks the cumulative effect of the row operations that converts $A$ to reduced row-echelon form, here effectively applying them to the vector of constants in a system of equations having $A$ as a coefficient matrix.  When $A$ row-reduces to a matrix with zero rows, then $J\vect{y}$ should also have zero entries in the same rows if the system is to be consistent.</p>
</div>
<div class="subsection" id="subsection-FS" acro="FS" titletext="Four Subsets">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">FS</span> <span class="titletext">Four Subsets</span>
</h4>
<p>With all the preliminaries in place we can state our main result for this section.  In essence this result will allow us to say that we can find linearly independent sets to use in span constructions for all four subsets (null space, column space, row space, left null space) by analyzing only the extended echelon form of the matrix, and specifically, just the two submatrices $C$ and $L$, which will be ripe for analysis since they are already in reduced row-echelon form (<a class="knowl" acro="PEEF" type="Theorem" title="Properties of Extended Echelon Form" knowl="./knowls/theorem.PEEF.knowl">Theorem PEEF</a>).</p>
<div class="theorem" id="theorem-FS" acro="FS" titletext="Four Subsets">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">FS</span><span class="titletext"> Four Subsets</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix with extended echelon form $N$.  Suppose the reduced row-echelon form of $A$ has $r$ nonzero rows.  Then $C$ is the submatrix of $N$ formed from the first $r$ rows and the first $n$ columns and $L$ is the submatrix of $N$ formed from the last $m$ columns and the last $m-r$ rows.  Then
<ol>
<li> The null space of $A$ is the null space of $C$, $\nsp{A}=\nsp{C}$.
</li>
<li> The row space of $A$ is the row space of $C$, $\rsp{A}=\rsp{C}$.
</li>
<li> The column space of $A$ is the null space of $L$, $\csp{A}=\nsp{L}$.
</li>
<li> The left null space of $A$ is the row space of $L$, $\lns{A}=\rsp{L}$.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.FS.knowl">Proof</a></div>
</div>
<p>The first two conclusions of this theorem are nearly trivial.  But they set up a pattern of results for $C$ that is reflected in the latter two conclusions about $L$.  In total, they tell us that we can compute all four subsets just by finding null spaces and row spaces.  This theorem does not tell us exactly how to compute these subsets, but instead simply expresses them as null spaces and row spaces of matrices in reduced row-echelon form without any zero rows ($C$ and $L$).   A linearly independent set that spans the null space of a matrix in reduced row-echelon form can be found easily with <a class="knowl" acro="BNS" type="Theorem" title="Basis for Null Spaces" knowl="./knowls/theorem.BNS.knowl">Theorem BNS</a>.  It is an even easier matter to find a linearly independent set that spans the row space of a matrix in reduced row-echelon form with <a class="knowl" acro="BRS" type="Theorem" title="Basis for the Row Space" knowl="./knowls/theorem.BRS.knowl">Theorem BRS</a>, especially when there are no zero rows present.  So an application of <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> is typically followed by two applications each of <a class="knowl" acro="BNS" type="Theorem" title="Basis for Null Spaces" knowl="./knowls/theorem.BNS.knowl">Theorem BNS</a> and <a class="knowl" acro="BRS" type="Theorem" title="Basis for the Row Space" knowl="./knowls/theorem.BRS.knowl">Theorem BRS</a>.</p>
<p>The situation when $r=m$ deserves comment, since now the matrix $L$ has no rows.  What is $\csp{A}$ when we try to apply <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> and encounter $\nsp{L}$?  One interpretation of this situation is that $L$ is the coefficient matrix of a homogeneous system that has no equations.  How hard is it to find a solution vector to this system?  Some thought will convince you that <em>any</em> proposed vector will qualify as a solution, since it makes <em>all</em> of the equations true.  So every possible vector is in the null space of $L$ and therefore $\csp{A}=\nsp{L}=\complex{m}$.  OK, perhaps this sounds like some twisted argument from <i>Alice in Wonderland</i>.  Let us try another argument that might solidly convince you of this logic.</p>
<p>If $r=m$, when we row-reduce the augmented matrix of $\linearsystem{A}{\vect{b}}$ the result will have no zero rows, and the first $n$ columns will all be pivot columns, leaving none for the final column, so by <a class="knowl" acro="RCLS" type="Theorem" title="Recognizing Consistency of a Linear System" knowl="./knowls/theorem.RCLS.knowl">Theorem RCLS</a> the system will be consistent.  By <a class="knowl" acro="CSCS" type="Theorem" title="Column Spaces and Consistent Systems" knowl="./knowls/theorem.CSCS.knowl">Theorem CSCS</a>, $\vect{b}\in\csp{A}$.  Since $\vect{b}$ was arbitrary, every possible vector is in the column space of $A$, so we again have $\csp{A}=\complex{m}$.  The situation when a matrix has $r=m$ is known by the term <em class="term">full rank</em>, and in the case of a square matrix coincides with nonsingularity (see <a knowl="./knowls/exercise.FS.M50.knowl">Exercise FS.M50</a>).</p>
<p>The properties of the matrix $L$ described by this theorem can be explained informally as follows.  A column vector $\vect{y}\in\complex{m}$ is in the column space of $A$ if the linear system $\linearsystem{A}{\vect{y}}$ is consistent (<a class="knowl" acro="CSCS" type="Theorem" title="Column Spaces and Consistent Systems" knowl="./knowls/theorem.CSCS.knowl">Theorem CSCS</a>).  By <a class="knowl" acro="RCLS" type="Theorem" title="Recognizing Consistency of a Linear System" knowl="./knowls/theorem.RCLS.knowl">Theorem RCLS</a>, the reduced row-echelon form of the augmented matrix $\augmented{A}{\vect{y}}$ of a consistent system will have zeros in the bottom $m-r$ locations of the last column.  By <a class="knowl" acro="PEEF" type="Theorem" title="Properties of Extended Echelon Form" knowl="./knowls/theorem.PEEF.knowl">Theorem PEEF</a> this final column is the vector $J\vect{y}$ and so should then have zeros in the final $m-r$ locations.  But since $L$ comprises the final $m-r$ rows of $J$, this condition is expressed by saying $\vect{y}\in\nsp{L}$.</p>
<p>Additionally, the rows of $J$ are the scalars in linear combinations of the rows of $A$ that create the rows of $B$.  That is, the rows of $J$ record the net effect of the sequence of row operations that takes $A$ to its reduced row-echelon form, $B$.  This can be seen in the equation $JA=B$ (<a class="knowl" acro="PEEF" type="Theorem" title="Properties of Extended Echelon Form" knowl="./knowls/theorem.PEEF.knowl">Theorem PEEF</a>).  As such, the rows of $L$ are scalars for linear combinations of the rows of $A$ that yield zero rows.  But such linear combinations are precisely the elements of the left null space.  So any element of the row space of $L$ is also an element of the left null space of $A$.</p>
<p>We will now illustrate <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> with a few examples.</p>
<div class="example" id="example-FS1" acro="FS1" titletext="Four subsets, no. 1"><h5 class="example">
<a knowl="./knowls/example.FS1.knowl"><span class="type">Example</span> <span class="acro">FS1</span></a> <span class="titletext">Four subsets, no. 1</span>
</h5></div>
<div class="example" id="example-FS2" acro="FS2" titletext="Four subsets, no. 2"><h5 class="example">
<a knowl="./knowls/example.FS2.knowl"><span class="type">Example</span> <span class="acro">FS2</span></a> <span class="titletext">Four subsets, no. 2</span>
</h5></div>
<p>The next example is just a bit different since the matrix has more rows than columns, and a trivial null space.</p>
<div class="example" id="example-FSAG" acro="FSAG" titletext="Four subsets, Archetype G"><h5 class="example">
<a knowl="./knowls/example.FSAG.knowl"><span class="type">Example</span> <span class="acro">FSAG</span></a> <span class="titletext">Four subsets, Archetype G</span>
</h5></div>
<div class="sage" id="sage-EEF" acro="EEF" titletext="Extended Echelon Form"><h5 class="sage">
<a knowl="./knowls/sage.EEF.knowl"><span class="type">Sage</span> <span class="acro">EEF</span></a> <span class="titletext">Extended Echelon Form</span>
</h5></div>
<p><a class="knowl" acro="COV" type="Example" title="Casting out vectors" knowl="./knowls/example.COV.knowl">Example COV</a> and <a class="knowl" acro="CSROI" type="Example" title="Column space from row operations, Archetype I" knowl="./knowls/example.CSROI.knowl">Example CSROI</a> each describes the column space of the coefficient matrix from <a knowl="./knowls/archetype.I.knowl">Archetype I</a> as the span of a set of $r=3$ linearly independent vectors.  It is no accident that these two different sets both have the same size.  If we (you?) were to calculate the column space of this matrix using the null space of the matrix $L$ from <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> then we would again find a set of 3 linearly independent vectors that span the range.  More on this later.</p>
<p>So we have three different methods to obtain a description of the column space of a matrix as the span of a linearly independent set.  <a class="knowl" acro="BCS" type="Theorem" title="Basis of the Column Space" knowl="./knowls/theorem.BCS.knowl">Theorem BCS</a> is sometimes useful since the vectors it specifies are equal to actual columns of the matrix. <a class="knowl" acro="BRS" type="Theorem" title="Basis for the Row Space" knowl="./knowls/theorem.BRS.knowl">Theorem BRS</a> and <a class="knowl" acro="CSRST" type="Theorem" title="Column Space, Row Space, Transpose" knowl="./knowls/theorem.CSRST.knowl">Theorem CSRST</a> combine to create vectors with lots of zeros, and strategically placed 1's near the top of the vector.   <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a> and the matrix $L$ from the extended echelon form gives us a third method, which tends to create vectors with lots of zeros, and strategically placed 1's near the bottom of the vector.   If we do not care about linear independence we can also appeal to <a class="knowl" acro="CSM" type="Definition" title="Column Space of a Matrix" knowl="./knowls/definition.CSM.knowl">Definition CSM</a> and simply express the column space as the span of all the columns of the matrix, giving us a fourth description.</p>
<p>With <a class="knowl" acro="CSRST" type="Theorem" title="Column Space, Row Space, Transpose" knowl="./knowls/theorem.CSRST.knowl">Theorem CSRST</a> and <a class="knowl" acro="RSM" type="Definition" title="Row Space of a Matrix" knowl="./knowls/definition.RSM.knowl">Definition RSM</a>, we can compute column spaces with theorems about row spaces, and we can compute row spaces with theorems about column spaces, but in each case we must transpose the matrix first.  At this point you may be overwhelmed by all the possibilities for computing column and row spaces.  <a class="knowl" acro="CSRST" type="Diagram" title="Column Space and Row Space Techniques" knowl="./knowls/diagram.CSRST.knowl">Diagram CSRST</a> is meant to help.  For both the column space and row space, it suggests four techniques.  One is to appeal to the definition, another yields a span of a linearly independent set, and a third uses <a class="knowl" acro="FS" type="Theorem" title="Four Subsets" knowl="./knowls/theorem.FS.knowl">Theorem FS</a>.  A fourth suggests transposing the matrix and the dashed line implies that then the companion set of techniques can be applied.  This can lead to a bit of silliness, since if you were to follow the dashed lines <em>twice</em> you would transpose the matrix twice, and by <a class="knowl" acro="TT" type="Theorem" title="Transpose of a Transpose" knowl="./knowls/theorem.TT.knowl">Theorem TT</a> would accomplish nothing productive.
<div class="diagram" id="diagram-CSRST" acro="CSRST" titletext="Column Space and Row Space Techniques">
<a id="diagram-CSRST"></a><object type="image/svg+xml" data="./diagrams/CSRST.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">CSRST</span> <span class="titletext">Column Space and Row Space Techniques</span>
</h5>
</div>
</p>
<p>Although we have many ways to describe a column space, notice that one tempting strategy will usually fail.  It is not possible to simply row-reduce a matrix directly and then use the columns of the row-reduced matrix as a set whose span equals the column space.  In other words, row operations <em>do not</em> preserve column spaces (however row operations do preserve row spaces, <a class="knowl" acro="REMRS" type="Theorem" title="Row-Equivalent Matrices have equal Row Spaces" knowl="./knowls/theorem.REMRS.knowl">Theorem REMRS</a>).  See <a knowl="./knowls/exercise.CRS.M21.knowl">Exercise CRS.M21</a>.</p>
</div>
<div class="readingquestions" id="readingquestions-FS" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.FS.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-FS" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.FS.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
