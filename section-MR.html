<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-R.html">Representations</a> » <a href="section-MR.html">Matrix Representations</a> »</div></div>
<div id="title"><span id="title-content">Matrix Representations</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-MR" acro="MR" titletext="Matrix Representations">
<h3 class="section">
<span class="type">Section</span> <span class="acro">MR</span> <span class="titletext">Matrix Representations</span>
</h3>
<div class="introduction">
<p>We have seen that linear transformations whose domain and codomain are vector spaces of columns vectors have a close relationship with matrices (<a class="knowl" acro="MBLT" type="Theorem" title="Matrices Build Linear Transformations" knowl="./knowls/theorem.MBLT.knowl">Theorem MBLT</a>, <a class="knowl" acro="MLTCV" type="Theorem" title="Matrix of a Linear Transformation, Column Vectors" knowl="./knowls/theorem.MLTCV.knowl">Theorem MLTCV</a>).  In this section, we will extend the relationship between matrices and linear transformations to the setting of linear transformations between abstract vector spaces.</p>

</div>
<div class="subsection" id="subsection-MR" acro="MR" titletext="Matrix Representations">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">MR</span> <span class="titletext">Matrix Representations</span>
</h4>
<p>This is a fundamental definition.</p>
<div class="definition" id="definition-MR" acro="MR" titletext="Matrix Representation">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">MR</span> <span class="titletext"> Matrix Representation</span>
</h5>
<p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B=\set{\vectorlist{u}{n}}$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$ of size $m$.  Then the <em class="term">matrix representation</em> of $T$ relative to $B$ and $C$ is the $m\times n$ matrix,
\begin{equation*}
\matrixrep{T}{B}{C}=\left[
\left.\vectrep{C}{\lt{T}{\vect{u}_1}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_2}}\right|
\left.\vectrep{C}{\lt{T}{\vect{u}_3}}\right|
\ldots
\left|\vectrep{C}{\lt{T}{\vect{u}_n}}\right.
\right]
\end{equation*}
</p>
</div>
<div class="example" id="example-OLTTR" acro="OLTTR" titletext="One linear transformation, three representations"><h5 class="example">
<a knowl="./knowls/example.OLTTR.knowl"><span class="type">Example</span> <span class="acro">OLTTR</span></a> <span class="titletext">One linear transformation, three representations</span>
</h5></div>
<p>We may choose to use whatever terms we want when we make a definition.  Some are arbitrary, while others make sense, but only in light of subsequent theorems.  Matrix representation is in the latter category.  We begin with a linear transformation and produce a matrix.  So what?  Here is the theorem that <em>justifies</em> the term “matrix representation.”</p>
<div class="theorem" id="theorem-FTMR" acro="FTMR" titletext="Fundamental Theorem of Matrix Representation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">FTMR</span><span class="titletext"> Fundamental Theorem of Matrix Representation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$, $C$ is a basis for $V$ and $\matrixrep{T}{B}{C}$ is the matrix representation of $T$ relative to $B$ and $C$.  Then, for any $\vect{u}\in U$,
\begin{equation*}
\vectrep{C}{\lt{T}{\vect{u}}}=\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)
\end{equation*}
or equivalently
\begin{equation*}
\lt{T}{\vect{u}}=\vectrepinv{C}{\matrixrep{T}{B}{C}\left(\vectrep{B}{\vect{u}}\right)}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.FTMR.knowl">Proof</a></div>
</div>
<p>This theorem says that we can apply $T$ to $\vect{u}$ and coordinatize the result relative to $C$ in $V$, or we can first coordinatize $\vect{u}$ relative to $B$ in $U$, then multiply by the matrix representation.  Either way, the result is the same.  So the effect of a linear transformation can always be accomplished by a matrix-vector product (<a class="knowl" acro="MVP" type="Definition" title="Matrix-Vector Product" knowl="./knowls/definition.MVP.knowl">Definition MVP</a>).  That is important enough to say again.  The effect of a linear transformation is a matrix-vector product.
<div class="diagram" id="diagram-FTMR" acro="FTMR" titletext="Fundamental Theorem of Matrix Representations">
<a id="diagram-FTMR"></a><object type="image/svg+xml" data="./diagrams/FTMR.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">FTMR</span> <span class="titletext">Fundamental Theorem of Matrix Representations</span>
</h5>
</div>
</p>
<p>The alternative conclusion of this result might be even more striking.  It says that to effect a linear transformation ($T$) of a vector ($\vect{u}$), coordinatize the input (with $\vectrepname{B}$), do a matrix-vector product (with $\matrixrep{T}{B}{C}$), and un-coordinatize the result (with $\vectrepinvname{C}$).  So, absent some bookkeeping about vector representations, a linear transformation <em>is</em> a matrix.  To adjust the diagram, we “reverse” the arrow on the right, which means inverting the vector representation $\vectrepname{C}$ on $V$.  Now we can go directly across the top of the diagram, computing the linear transformation between the abstract vector spaces.  Or, we can around the other three sides, using vector representation, a matrix-vector product, followed by un-coordinatization.
<div class="diagram" id="diagram-FTMRA" acro="FTMRA" titletext="Fundamental Theorem of Matrix Representations (Alternate)">
<a id="diagram-FTMRA"></a><object type="image/svg+xml" data="./diagrams/FTMRA.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">FTMRA</span> <span class="titletext">Fundamental Theorem of Matrix Representations (Alternate)</span>
</h5>
</div>
</p>
<p>Here is an example to illustrate how the “action” of a linear transformation can be effected by matrix multiplication.</p>
<div class="example" id="example-ALTMM" acro="ALTMM" titletext="A linear transformation as matrix multiplication"><h5 class="example">
<a knowl="./knowls/example.ALTMM.knowl"><span class="type">Example</span> <span class="acro">ALTMM</span></a> <span class="titletext">A linear transformation as matrix multiplication</span>
</h5></div>
<p>We will use <a class="knowl" acro="FTMR" type="Theorem" title="Fundamental Theorem of Matrix Representation" knowl="./knowls/theorem.FTMR.knowl">Theorem FTMR</a> frequently in the next few sections.  A typical application will feel like the linear transformation $T$ “commutes” with a vector representation, $\vectrepname{C}$, and as it does the transformation morphs into a matrix, $\matrixrep{T}{B}{C}$, while the vector representation changes to a new basis, $\vectrepname{B}$.  Or vice-versa.</p>
<div class="sage" id="sage-MR" acro="MR" titletext="Matrix Representations"><h5 class="sage">
<a knowl="./knowls/sage.MR.knowl"><span class="type">Sage</span> <span class="acro">MR</span></a> <span class="titletext">Matrix Representations</span>
</h5></div>
</div>
<div class="subsection" id="subsection-NRFO" acro="NRFO" titletext="New Representations from Old">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">NRFO</span> <span class="titletext">New Representations from Old</span>
</h4>
<p>In <a class="knowl" acro="LT.NLTFO" type="Subsection" title="" knowl="./knowls/subsection.LT.NLTFO.knowl">Subsection LT.NLTFO</a> we built new linear transformations from other linear transformations.  Sums, scalar multiples and compositions.  These new linear transformations will have matrix representations as well.  How do the new matrix representations relate to the old matrix representations?  Here are the three theorems.</p>
<div class="theorem" id="theorem-MRSLT" acro="MRSLT" titletext="Matrix Representation of a Sum of Linear Transformations">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MRSLT</span><span class="titletext"> Matrix Representation of a Sum of Linear Transformations</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{U}{V}$ are linear transformations, $B$ is a basis of $U$ and $C$ is a basis of $V$.  Then
\begin{equation*}
\matrixrep{T+S}{B}{C}=\matrixrep{T}{B}{C}+\matrixrep{S}{B}{C}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.MRSLT.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-MRMLT" acro="MRMLT" titletext="Matrix Representation of a Multiple of a Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MRMLT</span><span class="titletext"> Matrix Representation of a Multiple of a Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $\alpha\in\complex{\null}$, $B$ is a basis of $U$ and $C$ is a basis of $V$.  Then
\begin{equation*}
\matrixrep{\alpha T}{B}{C}=\alpha\matrixrep{T}{B}{C}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.MRMLT.knowl">Proof</a></div>
</div>
<p>The vector space of all linear transformations from $U$ to $V$ is now isomorphic to the vector space of all $m\times n$ matrices.</p>
<div class="theorem" id="theorem-MRCLT" acro="MRCLT" titletext="Matrix Representation of a Composition of Linear Transformations">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MRCLT</span><span class="titletext"> Matrix Representation of a Composition of Linear Transformations</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ and $\ltdefn{S}{V}{W}$ are linear transformations, $B$ is a basis of $U$, $C$ is a basis of $V$, and $D$ is a basis of $W$.  Then
\begin{equation*}
\matrixrep{\compose{S}{T}}{B}{D}=\matrixrep{S}{C}{D}\matrixrep{T}{B}{C}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.MRCLT.knowl">Proof</a></div>
</div>
<p>This is the second great surprise of introductory linear algebra.  Matrices are linear transformations (functions, really), and matrix multiplication is function composition!  We can form the composition of two linear transformations, then form the matrix representation of the result.  Or we can form the matrix representation of each linear transformation separately, then <em>multiply</em> the two representations together via <a class="knowl" acro="MM" type="Definition" title="Matrix Multiplication" knowl="./knowls/definition.MM.knowl">Definition MM</a>.  In either case, we arrive at the same result.</p>
<div class="example" id="example-MPMR" acro="MPMR" titletext="Matrix product of matrix representations"><h5 class="example">
<a knowl="./knowls/example.MPMR.knowl"><span class="type">Example</span> <span class="acro">MPMR</span></a> <span class="titletext">Matrix product of matrix representations</span>
</h5></div>
<p>A diagram, similar to ones we have seen earlier, might make the importance of this theorem clearer,
<div class="diagram" id="diagram-MRCLT" acro="MRCLT" titletext="Matrix Representation and Composition of Linear Transformations">
<a id="diagram-MRCLT"></a><object type="image/svg+xml" data="./diagrams/MRCLT.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">MRCLT</span> <span class="titletext">Matrix Representation and Composition of Linear Transformations</span>
</h5>
</div>
</p>
<p>One of our goals in the first part of this book is to make the definition of matrix multiplication (<a class="knowl" acro="MVP" type="Definition" title="Matrix-Vector Product" knowl="./knowls/definition.MVP.knowl">Definition MVP</a>, <a class="knowl" acro="MM" type="Definition" title="Matrix Multiplication" knowl="./knowls/definition.MM.knowl">Definition MM</a>) seem as natural as possible.  However, many of us are brought up with an entry-by-entry description of matrix multiplication (<a class="knowl" acro="EMP" type="Theorem" title="Entries of Matrix Products" knowl="./knowls/theorem.EMP.knowl">Theorem EMP</a>) as the <em>definition</em> of matrix multiplication, and then theorems about columns of matrices and linear combinations follow from that definition.  With this unmotivated definition, the realization that matrix multiplication is function composition is quite remarkable.  It is an interesting exercise to begin with the question, “What is the matrix representation of the composition of two linear transformations?” and then, without using any theorems about matrix multiplication, finally arrive at the entry-by-entry description of matrix multiplication.  Try it yourself (<a knowl="./knowls/exercise.MR.T80.knowl">Exercise MR.T80</a>).</p>
<div class="sage" id="sage-SUTH3" acro="SUTH3" titletext="Sage Under The Hood, Round 3"><h5 class="sage">
<a knowl="./knowls/sage.SUTH3.knowl"><span class="type">Sage</span> <span class="acro">SUTH3</span></a> <span class="titletext">Sage Under The Hood, Round 3</span>
</h5></div>
</div>
<div class="subsection" id="subsection-PMR" acro="PMR" titletext="Properties of Matrix Representations">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">PMR</span> <span class="titletext">Properties of Matrix Representations</span>
</h4>
<p>It will not be a surprise to discover that the kernel and range of a linear transformation are closely related to the null space and column space of the transformation's matrix representation.  Perhaps this idea has been bouncing around in your head already, even before seeing the definition of a matrix representation.  However, with a formal definition of a matrix representation (<a class="knowl" acro="MR" type="Definition" title="Matrix Representation" knowl="./knowls/definition.MR.knowl">Definition MR</a>), and a fundamental theorem to go with it (<a class="knowl" acro="FTMR" type="Theorem" title="Fundamental Theorem of Matrix Representation" knowl="./knowls/theorem.FTMR.knowl">Theorem FTMR</a>) we can be formal about the relationship, using the idea of isomorphic vector spaces (<a class="knowl" acro="IVS" type="Definition" title="Isomorphic Vector Spaces" knowl="./knowls/definition.IVS.knowl">Definition IVS</a>).  Here are the twin theorems.</p>
<div class="theorem" id="theorem-KNSI" acro="KNSI" titletext="Kernel and Null Space Isomorphism">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">KNSI</span><span class="titletext"> Kernel and Null Space Isomorphism</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$.  Then the kernel of $T$ is isomorphic to the null space of $\matrixrep{T}{B}{C}$,
\begin{equation*}
\krn{T}\isomorphic\nsp{\matrixrep{T}{B}{C}}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.KNSI.knowl">Proof</a></div>
</div>
<div class="example" id="example-KVMR" acro="KVMR" titletext="Kernel via matrix representation"><h5 class="example">
<a knowl="./knowls/example.KVMR.knowl"><span class="type">Example</span> <span class="acro">KVMR</span></a> <span class="titletext">Kernel via matrix representation</span>
</h5></div>
<p>An entirely similar result applies to the range of a linear transformation and the column space of a matrix representation of the linear transformation.</p>
<div class="theorem" id="theorem-RCSI" acro="RCSI" titletext="Range and Column Space Isomorphism">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">RCSI</span><span class="titletext"> Range and Column Space Isomorphism</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ of size $n$, and $C$ is a basis for $V$ of size $m$.  Then the range of $T$ is isomorphic to the column space of $\matrixrep{T}{B}{C}$,
\begin{equation*}
\rng{T}\isomorphic\csp{\matrixrep{T}{B}{C}}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.RCSI.knowl">Proof</a></div>
</div>
<div class="example" id="example-RVMR" acro="RVMR" titletext="Range via matrix representation"><h5 class="example">
<a knowl="./knowls/example.RVMR.knowl"><span class="type">Example</span> <span class="acro">RVMR</span></a> <span class="titletext">Range via matrix representation</span>
</h5></div>
<p><a class="knowl" acro="KNSI" type="Theorem" title="Kernel and Null Space Isomorphism" knowl="./knowls/theorem.KNSI.knowl">Theorem KNSI</a> and <a class="knowl" acro="RCSI" type="Theorem" title="Range and Column Space Isomorphism" knowl="./knowls/theorem.RCSI.knowl">Theorem RCSI</a> can be viewed as further formal evidence for the <a href="section-VR.html" title="Coordinatization Principle">Coordinatization Principle</a>, though they are not direct consequences.</p>
<p><a class="knowl" acro="KRI" type="Diagram" title="Kernel and Range Isomorphisms" knowl="./knowls/diagram.KRI.knowl">Diagram KRI</a> is meant to suggest <a class="knowl" acro="KNSI" type="Theorem" title="Kernel and Null Space Isomorphism" knowl="./knowls/theorem.KNSI.knowl">Theorem KNSI</a> and <a class="knowl" acro="RCSI" type="Theorem" title="Range and Column Space Isomorphism" knowl="./knowls/theorem.RCSI.knowl">Theorem RCSI</a>, in addition to their proofs (and so carry the same notation as the statements of these two theorems).  The dashed lines indicate a subspace relationship, with the smaller vector space lower down in the diagram.  The central square is highly reminiscent of <a class="knowl" acro="FTMR" type="Diagram" title="Fundamental Theorem of Matrix Representations" knowl="./knowls/diagram.FTMR.knowl">Diagram FTMR</a>.  Each of the four vector representations is an isomorphism, so the inverse linear transformation could be depicted with an arrow pointing in the other direction.  The four vector spaces across the bottom are familiar from the earliest days of the course, while the four vector spaces across the top are completely abstract.  The vector representations that are restrictions (far left and far right) are the functions shown to be invertible representations as the key technique in the proofs of <a class="knowl" acro="KNSI" type="Theorem" title="Kernel and Null Space Isomorphism" knowl="./knowls/theorem.KNSI.knowl">Theorem KNSI</a> and <a class="knowl" acro="RCSI" type="Theorem" title="Range and Column Space Isomorphism" knowl="./knowls/theorem.RCSI.knowl">Theorem RCSI</a>.  So this diagram could be helpful as you study those two proofs.
<div class="diagram" id="diagram-KRI" acro="KRI" titletext="Kernel and Range Isomorphisms">
<a id="diagram-KRI"></a><object type="image/svg+xml" data="./diagrams/KRI.svg">SVG image not dispayed</object><br><br><h5 class="diagram">
<span class="type">Diagram</span> <span class="acro">KRI</span> <span class="titletext">Kernel and Range Isomorphisms</span>
</h5>
</div>
</p>
<div class="sage" id="sage-LTR" acro="LTR" titletext="Linear Transformation Restrictions"><h5 class="sage">
<a knowl="./knowls/sage.LTR.knowl"><span class="type">Sage</span> <span class="acro">LTR</span></a> <span class="titletext">Linear Transformation Restrictions</span>
</h5></div>
</div>
<div class="subsection" id="subsection-IVLT" acro="IVLT" titletext="Invertible Linear Transformations">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">IVLT</span> <span class="titletext">Invertible Linear Transformations</span>
</h4>
<p>We have seen, both in theorems and in examples, that questions about linear transformations are often equivalent to questions about matrices.  It is the matrix representation of a linear transformation that makes this idea precise.  Here is our final theorem that solidifies this connection.</p>
<div class="theorem" id="theorem-IMR" acro="IMR" titletext="Invertible Matrix Representations">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">IMR</span><span class="titletext"> Invertible Matrix Representations</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{U}{V}$ is a linear transformation, $B$ is a basis for $U$ and $C$ is a basis for $V$. Then $T$ is an invertible linear transformation if and only if the matrix representation of $T$ relative to $B$ and $C$, $\matrixrep{T}{B}{C}$ is an invertible matrix.  When $T$ is invertible,
\begin{equation*}
\matrixrep{\ltinverse{T}}{C}{B}=\inverse{\left(\matrixrep{T}{B}{C}\right)}
\end{equation*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.IMR.knowl">Proof</a></div>
</div>
<p>By now, the connections between matrices and linear transformations should be starting to become more transparent, and you may have already recognized the invertibility of a matrix as being  tantamount to the invertibility of the associated matrix representation.  The next example shows how to apply this theorem to the problem of actually building a formula for the inverse of an invertible linear transformation.</p>
<div class="example" id="example-ILTVR" acro="ILTVR" titletext="Inverse of a linear transformation via a representation"><h5 class="example">
<a knowl="./knowls/example.ILTVR.knowl"><span class="type">Example</span> <span class="acro">ILTVR</span></a> <span class="titletext">Inverse of a linear transformation via a representation</span>
</h5></div>
<p>You might look back at <a class="knowl" acro="AIVLT" type="Example" title="An invertible linear transformation" knowl="./knowls/example.AIVLT.knowl">Example AIVLT</a>, where we first witnessed the inverse of a linear transformation and recognize that the inverse ($S$) was built from using the method of <a class="knowl" acro="ILTVR" type="Example" title="Inverse of a linear transformation via a representation" knowl="./knowls/example.ILTVR.knowl">Example ILTVR</a> with a matrix representation of $T$.</p>
<div class="theorem" id="theorem-IMILT" acro="IMILT" titletext="Invertible Matrices, Invertible Linear Transformation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">IMILT</span><span class="titletext"> Invertible Matrices, Invertible Linear Transformation</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix of size $n$ and $\ltdefn{T}{\complex{n}}{\complex{n}}$ is the linear transformation defined by $\lt{T}{\vect{x}}=A\vect{x}$.  Then $A$ is an invertible matrix if and only if $T$ is an invertible linear transformation.</p></div>
<div class="proof"><a knowl="./knowls/proof.IMILT.knowl">Proof</a></div>
</div>
<p>This theorem may seem gratuitous.  Why state such a special case of <a class="knowl" acro="IMR" type="Theorem" title="Invertible Matrix Representations" knowl="./knowls/theorem.IMR.knowl">Theorem IMR</a>?  Because it adds another condition to our NMEx series of theorems, and in some ways it is the most fundamental expression of what it means for a matrix to be nonsingular — the associated linear transformation is invertible.  This is our final update.</p>
<div class="theorem" id="theorem-NME9" acro="NME9" titletext="Nonsingular Matrix Equivalences, Round 9">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NME9</span><span class="titletext"> Nonsingular Matrix Equivalences, Round 9</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
<ol>
<li> $A$ is nonsingular.
</li>
<li> $A$ row-reduces to the identity matrix.
</li>
<li> The null space of $A$ contains only the zero vector, $\nsp{A}=\set{\zerovector}$.
</li>
<li> The linear system $\linearsystem{A}{\vect{b}}$ has a unique solution for every possible choice of $\vect{b}$.
</li>
<li> The columns of $A$ are a linearly independent set.
</li>
<li> $A$ is invertible.
</li>
<li> The column space of $A$ is $\complex{n}$, $\csp{A}=\complex{n}$.
</li>
<li> The columns of $A$ are a basis for $\complex{n}$.
</li>
<li> The rank of $A$ is $n$, $\rank{A}=n$.
</li>
<li> The nullity of $A$ is zero, $\nullity{A}=0$.
</li>
<li> The determinant of $A$ is nonzero, $\detname{A}\neq 0$.
</li>
<li> $\lambda=0$ is not an eigenvalue of $A$.
</li>
<li> The linear transformation $\ltdefn{T}{\complex{n}}{\complex{n}}$ defined by $\lt{T}{\vect{x}}=A\vect{x}$ is invertible.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.NME9.knowl">Proof</a></div>
</div>
<div class="sage" id="sage-NME9" acro="NME9" titletext="Nonsingular Matrix Equivalences, Round 9"><h5 class="sage">
<a knowl="./knowls/sage.NME9.knowl"><span class="type">Sage</span> <span class="acro">NME9</span></a> <span class="titletext">Nonsingular Matrix Equivalences, Round 9</span>
</h5></div>
</div>
<div class="readingquestions" id="readingquestions-MR" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.MR.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-MR" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.MR.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
