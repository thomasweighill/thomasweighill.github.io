<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-M.html">Matrices</a> » <a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a> »</div></div>
<div id="title"><span id="title-content">Matrix Inverses and Nonsingular Matrices</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-MINM" acro="MINM" titletext="Matrix Inverses and Nonsingular Matrices">
<h3 class="section">
<span class="type">Section</span> <span class="acro">MINM</span> <span class="titletext">Matrix Inverses and Nonsingular Matrices</span>
</h3>
<div class="introduction">
<p>We saw in <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a> that if a square matrix $A$ is nonsingular, then there is a matrix $B$ so that $AB=I_n$.  In other words, $B$ is halfway to being an inverse of $A$.  We will see in this section that $B$ automatically fulfills the second condition ($BA=I_n$).  <a class="knowl" acro="MWIAA" type="Example" title="A matrix without an inverse, Archetype A" knowl="./knowls/example.MWIAA.knowl">Example MWIAA</a> showed us that the coefficient matrix from <a knowl="./knowls/archetype.A.knowl">Archetype A</a> had no inverse.  Not coincidentally, this coefficient matrix is singular.  We will make all these connections precise now.  Not many examples or definitions in this section, just theorems.</p>

</div>
<div class="subsection" id="subsection-NMI" acro="NMI" titletext="Nonsingular Matrices are Invertible">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">NMI</span> <span class="titletext">Nonsingular Matrices are Invertible</span>
</h4>
<p>We need a couple of technical results for starters.  Some books would call these minor, but essential, results “lemmas.”  We'll just call 'em theorems.  See <a class="knowl" acro="LC" type="Proof Technique" title="Lemmas and Corollaries" knowl="./knowls/technique.LC.knowl">Proof Technique LC</a> for more on the distinction.</p>
<p>The first of these technical results is interesting in that the hypothesis says something about a product of two square matrices and the conclusion then says the same thing about each individual matrix in the product.  This result has an analogy in the algebra of complex numbers: suppose $\alpha,\,\beta\in\complexes$, then $\alpha\beta\neq 0$ if and only if $\alpha\neq 0$ and $\beta\neq 0$.  We can view this result as suggesting that the term “nonsingular” for matrices is like the term “nonzero” for scalars.  Consider too that we know singular matrices, as coefficient matrices for systems of equations, will sometimes lead to systems with no solutions, or systems with infinitely many solutions (<a class="knowl" acro="NMUS" type="Theorem" title="Nonsingular Matrices and Unique Solutions" knowl="./knowls/theorem.NMUS.knowl">Theorem NMUS</a>).  What do linear equations with zero look like?  Consider $0x=5$, which has no solution, and $0x=0$, which has infinitely many solutions.  In the algebra of scalars, zero is exceptional (meaning different, not better), and in the algebra of matrices, singular matrices are also the exception.  While there is only one zero scalar, and there are infinitely many singular matrices, we will see that singular matrices are a distinct minority.</p>
<div class="theorem" id="theorem-NPNT" acro="NPNT" titletext="Nonsingular Product has Nonsingular Terms">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NPNT</span><span class="titletext"> Nonsingular Product has Nonsingular Terms</span>
</h5>
<div class="statement"><p>Suppose that $A$ and $B$ are square matrices of size $n$.  The product $AB$ is nonsingular if and only if $A$ and $B$ are both nonsingular.</p></div>
<div class="proof"><a knowl="./knowls/proof.NPNT.knowl">Proof</a></div>
</div>
<p>This is a powerful result in the “forward” direction, because it allows us to begin with a hypothesis that something complicated (the matrix product $AB$) has the property of being nonsingular, and we can then conclude that the simpler constituents ($A$ and $B$ individually) then also have the property of being nonsingular.  If we had thought that the matrix product was an artificial construction, results like this would make us begin to think twice.</p>
<p>The contrapositive of this entire result is equally interesting.  It says that $A$ or $B$ (or both) is a singular matrix if and only if the product $AB$ is singular.  (See <a class="knowl" acro="CP" type="Proof Technique" title="Contrapositives" knowl="./knowls/technique.CP.knowl">Proof Technique CP</a>.)</p>
<div class="theorem" id="theorem-OSIS" acro="OSIS" titletext="One-Sided Inverse is Sufficient">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">OSIS</span><span class="titletext"> One-Sided Inverse is Sufficient</span>
</h5>
<div class="statement"><p>Suppose $A$ and $B$ are  square matrices of size $n$ such that $AB=I_n$.  Then $BA=I_n$.</p></div>
<div class="proof"><a knowl="./knowls/proof.OSIS.knowl">Proof</a></div>
</div>
<p>So <a class="knowl" acro="OSIS" type="Theorem" title="One-Sided Inverse is Sufficient" knowl="./knowls/theorem.OSIS.knowl">Theorem OSIS</a> tells us that if $A$ is nonsingular, then the matrix $B$ guaranteed by <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a> will be both a “right-inverse” and a “left-inverse” for $A$, so $A$ is invertible and $\inverse{A}=B$.</p>
<p>So if you have a nonsingular matrix, $A$, you can use the procedure described in <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a> to find an inverse for $A$.  If $A$ is singular, then the procedure in <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a> will fail as the first $n$ columns of $M$ will not row-reduce to the identity matrix.  However, we can say a bit more.  When $A$ is singular, then $A$ does not have an inverse (which is very different from saying that the procedure in <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a> fails to find an inverse).
This may feel like we are splitting hairs, but it is important that we do not make unfounded assumptions.  These observations motivate the next theorem.</p>
<div class="theorem" id="theorem-NI" acro="NI" titletext="Nonsingularity is Invertibility">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NI</span><span class="titletext"> Nonsingularity is Invertibility</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix.  Then $A$ is nonsingular if and only if $A$ is invertible.</p></div>
<div class="proof"><a knowl="./knowls/proof.NI.knowl">Proof</a></div>
</div>
<p>So for a square matrix, the properties of having an inverse and of having a trivial null space are one and the same.  Cannot have one without the other.</p>
<div class="theorem" id="theorem-NME3" acro="NME3" titletext="Nonsingular Matrix Equivalences, Round 3">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NME3</span><span class="titletext"> Nonsingular Matrix Equivalences, Round 3</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
<ol>
<li> $A$ is nonsingular.
</li>
<li> $A$ row-reduces to the identity matrix.
</li>
<li> The null space of $A$ contains only the zero vector, $\nsp{A}=\set{\zerovector}$.
</li>
<li> The linear system $\linearsystem{A}{\vect{b}}$ has a unique solution for every possible choice of $\vect{b}$.
</li>
<li> The columns of $A$ are a linearly independent set.
</li>
<li> $A$ is invertible.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.NME3.knowl">Proof</a></div>
</div>
<p>In the case that $A$ is a nonsingular coefficient matrix of a system of equations, the inverse allows us to very quickly compute the unique solution, for any vector of constants.</p>
<div class="theorem" id="theorem-SNCM" acro="SNCM" titletext="Solution with Nonsingular Coefficient Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">SNCM</span><span class="titletext"> Solution with Nonsingular Coefficient Matrix</span>
</h5>
<div class="statement"><p>Suppose that $A$ is nonsingular.  Then the unique solution to $\linearsystem{A}{\vect{b}}$ is $\inverse{A}\vect{b}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.SNCM.knowl">Proof</a></div>
</div>
<div class="sage" id="sage-MI" acro="MI" titletext="Matrix Inverse"><h5 class="sage">
<a knowl="./knowls/sage.MI.knowl"><span class="type">Sage</span> <span class="acro">MI</span></a> <span class="titletext">Matrix Inverse</span>
</h5></div>
<div class="sage" id="sage-NME3" acro="NME3" titletext="Nonsingular Matrix Equivalences, Round 3"><h5 class="sage">
<a knowl="./knowls/sage.NME3.knowl"><span class="type">Sage</span> <span class="acro">NME3</span></a> <span class="titletext">Nonsingular Matrix Equivalences, Round 3</span>
</h5></div>
</div>
<div class="subsection" id="subsection-UM" acro="UM" titletext="Unitary Matrices">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">UM</span> <span class="titletext">Unitary Matrices</span>
</h4>
<p>Recall that the adjoint of a matrix is $\adjoint{A}=\transpose{\left(\conjugate{A}\right)}$ (<a class="knowl" acro="A" type="Definition" title="Adjoint" knowl="./knowls/definition.A.knowl">Definition A</a>).</p>
<div class="definition" id="definition-UM" acro="UM" titletext="Unitary Matrices">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">UM</span> <span class="titletext"> Unitary Matrices</span>
</h5>
<p>Suppose that $U$ is a square matrix of size $n$ such that $\adjoint{U}U=I_n$.  Then we say $U$ is <em class="term">unitary</em>.</p>
</div>
<p>This condition may seem rather far-fetched at first glance.  Would there be <em>any</em> matrix that behaved this way?  Well, yes, here is one.</p>
<div class="example" id="example-UM3" acro="UM3" titletext="Unitary matrix of size 3"><h5 class="example">
<a knowl="./knowls/example.UM3.knowl"><span class="type">Example</span> <span class="acro">UM3</span></a> <span class="titletext">Unitary matrix of size 3</span>
</h5></div>
<p>Unitary matrices do not have to look quite so gruesome.  Here is a larger one that is a bit more pleasing.</p>
<div class="example" id="example-UPM" acro="UPM" titletext="Unitary permutation matrix"><h5 class="example">
<a knowl="./knowls/example.UPM.knowl"><span class="type">Example</span> <span class="acro">UPM</span></a> <span class="titletext">Unitary permutation matrix</span>
</h5></div>
<p>If a matrix $A$ has only real number entries (we say it is a <em class="term">real matrix</em>) then the defining property of being unitary simplifies to $\transpose{A}A=I_n$.  In this case we, and everybody else, call the matrix <em class="term">orthogonal</em>, so you may often encounter this term in your other reading when the complex numbers are not under consideration.</p>
<p>Unitary matrices have easily computed inverses.  They also have columns that form orthonormal sets.  Here are the theorems that show us that unitary matrices are not as strange as they might initially appear.</p>
<div class="theorem" id="theorem-UMI" acro="UMI" titletext="Unitary Matrices are Invertible">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">UMI</span><span class="titletext"> Unitary Matrices are Invertible</span>
</h5>
<div class="statement"><p>Suppose that $U$ is a unitary matrix of size $n$.  Then $U$ is nonsingular, and $\inverse{U}=\adjoint{U}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.UMI.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-CUMOS" acro="CUMOS" titletext="Columns of Unitary Matrices are Orthonormal Sets">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">CUMOS</span><span class="titletext"> Columns of Unitary Matrices are Orthonormal Sets</span>
</h5>
<div class="statement"><p>Suppose that $S=\set{\vectorlist{A}{n}}$ is the set of columns of a square matrix $A$ of size $n$.  Then $A$ is a unitary matrix if and only if $S$ is an orthonormal set.</p></div>
<div class="proof"><a knowl="./knowls/proof.CUMOS.knowl">Proof</a></div>
</div>
<div class="example" id="example-OSMC" acro="OSMC" titletext="Orthonormal set from matrix columns"><h5 class="example">
<a knowl="./knowls/example.OSMC.knowl"><span class="type">Example</span> <span class="acro">OSMC</span></a> <span class="titletext">Orthonormal set from matrix columns</span>
</h5></div>
<p>When using vectors and matrices that only have real number entries, orthogonal matrices are those matrices with inverses that equal their transpose.  Similarly, the inner product is the familiar dot product.  Keep this special case in mind as you read the next theorem.</p>
<div class="theorem" id="theorem-UMPIP" acro="UMPIP" titletext="Unitary Matrices Preserve Inner Products">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">UMPIP</span><span class="titletext"> Unitary Matrices Preserve Inner Products</span>
</h5>
<div class="statement"><p>Suppose that $U$ is a unitary matrix of size $n$ and $\vect{u}$ and $\vect{v}$ are two vectors from $\complex{n}$.  Then
\begin{align*}
\innerproduct{U\vect{u}}{U\vect{v}}&amp;=\innerproduct{\vect{u}}{\vect{v}}
&amp;
&amp;\text{and}
&amp;
\norm{U\vect{v}}&amp;=\norm{\vect{v}}
\end{align*}

</p></div>
<div class="proof"><a knowl="./knowls/proof.UMPIP.knowl">Proof</a></div>
</div>
<p>Aside from the inherent interest in this theorem, it makes a bigger statement about unitary matrices.  When we view vectors geometrically as directions or forces, then the norm equates to a notion of length.  If we transform a vector by multiplication with a unitary matrix, then the length (norm) of that vector stays the same.  If we consider column vectors with two or three slots containing only real numbers, then the inner product of two such vectors is just the dot product, and this quantity can be used to compute the angle between two vectors.  When two vectors are multiplied (transformed) by the same unitary matrix, their dot product is unchanged and their individual lengths are unchanged.  This results in the angle between the two vectors remaining unchanged.</p>
<p>A “unitary transformation” (matrix-vector products with unitary matrices) thus preserve geometrical relationships among vectors representing directions, forces, or other physical quantities.  In the case of a two-slot vector with real entries, this is simply a rotation.  These sorts of computations are exceedingly important in computer graphics such as games and real-time simulations, especially when increased realism is achieved by performing many such computations quickly.  We will see unitary matrices again in subsequent sections (especially <a class="knowl" acro="OD" type="Theorem" title="Orthonormal Diagonalization" knowl="./knowls/theorem.OD.knowl">Theorem OD</a>) and in each instance, consider the interpretation of the unitary matrix as a sort of geometry-preserving transformation.  Some authors use the term <em class="term">isometry</em> to highlight this behavior.  We will speak loosely of a unitary matrix as being a sort of generalized rotation.</p>
<div class="sage" id="sage-UM" acro="UM" titletext="Unitary Matrices"><h5 class="sage">
<a knowl="./knowls/sage.UM.knowl"><span class="type">Sage</span> <span class="acro">UM</span></a> <span class="titletext">Unitary Matrices</span>
</h5></div>
<p>A final reminder:  the terms “dot product,” “symmetric matrix” and “orthogonal matrix” used in reference to vectors or matrices with real number entries are special cases of the terms “inner product,” “Hermitian matrix” and “unitary matrix” that we use for vectors or matrices with complex number entries, so keep that in mind as you read elsewhere.</p>
</div>
<div class="readingquestions" id="readingquestions-MINM" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.MINM.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-MINM" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.MINM.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
