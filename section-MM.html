<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-M.html">Matrices</a> » <a href="section-MM.html">Matrix Multiplication</a> »</div></div>
<div id="title"><span id="title-content">Matrix Multiplication</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-MM" acro="MM" titletext="Matrix Multiplication">
<h3 class="section">
<span class="type">Section</span> <span class="acro">MM</span> <span class="titletext">Matrix Multiplication</span>
</h3>
<div class="introduction">
<p>We know how to add vectors and how to multiply them by scalars.  Together, these operations give us the possibility of making linear combinations.  Similarly, we know how to add matrices and how to multiply matrices by scalars.  In this section we mix all these ideas together and produce an operation known as “matrix multiplication.”  This will lead to some results that are both surprising and central.  We begin with a definition of how to multiply a vector by a matrix.</p>

</div>
<div class="subsection" id="subsection-MVP" acro="MVP" titletext="Matrix-Vector Product">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">MVP</span> <span class="titletext">Matrix-Vector Product</span>
</h4>
<p>We have repeatedly seen the importance of forming linear combinations of the columns of a matrix.  As one example of this, the oft-used <a class="knowl" acro="SLSLC" type="Theorem" title="Solutions to Linear Systems are Linear Combinations" knowl="./knowls/theorem.SLSLC.knowl">Theorem SLSLC</a>, said that every solution to a system of linear equations gives rise to a linear combination of the column vectors of the coefficient matrix that equals the vector of constants.  This theorem, and others, motivate the following central definition.</p>
<div class="definition" id="definition-MVP" acro="MVP" titletext="Matrix-Vector Product">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">MVP</span> <span class="titletext"> Matrix-Vector Product</span>
</h5>
<p>Suppose $A$ is an $m\times n$ matrix with columns $\vectorlist{A}{n}$ and $\vect{u}$ is a vector of size $n$.  Then the <em class="term">matrix-vector product</em> of $A$ with $\vect{u}$ is the linear combination
\begin{equation*}
A\vect{u}=
\vectorentry{\vect{u}}{1}\vect{A}_1+
\vectorentry{\vect{u}}{2}\vect{A}_2+
\vectorentry{\vect{u}}{3}\vect{A}_3+
\cdots+
\vectorentry{\vect{u}}{n}\vect{A}_n
\end{equation*}
</p>
</div>
<p>So, the matrix-vector product is yet another version of “multiplication,” at least in the sense that we have yet again overloaded juxtaposition of two symbols as our notation.  Remember your objects, an $m\times n$ matrix times a vector of size $n$ will create a vector of size $m$.  So if $A$ is rectangular, then the size of the vector changes.  With all the linear combinations we have performed so far, this computation should now seem second nature.</p>
<div class="example" id="example-MTV" acro="MTV" titletext="A matrix times a vector"><h5 class="example">
<a knowl="./knowls/example.MTV.knowl"><span class="type">Example</span> <span class="acro">MTV</span></a> <span class="titletext">A matrix times a vector</span>
</h5></div>
<p>We can now represent systems of linear equations compactly with a matrix-vector product (<a class="knowl" acro="MVP" type="Definition" title="Matrix-Vector Product" knowl="./knowls/definition.MVP.knowl">Definition MVP</a>) and column vector equality (<a class="knowl" acro="CVE" type="Definition" title="Column Vector Equality" knowl="./knowls/definition.CVE.knowl">Definition CVE</a>).  This finally yields a very popular alternative to our unconventional $\linearsystem{A}{\vect{b}}$ notation.</p>
<div class="theorem" id="theorem-SLEMM" acro="SLEMM" titletext="Systems of Linear Equations as Matrix Multiplication">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">SLEMM</span><span class="titletext"> Systems of Linear Equations as Matrix Multiplication</span>
</h5>
<div class="statement"><p>The set of solutions to the linear system $\linearsystem{A}{\vect{b}}$ equals the set of solutions for $\vect{x}$ in the vector equation $A\vect{x}=\vect{b}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.SLEMM.knowl">Proof</a></div>
</div>
<div class="example" id="example-MNSLE" acro="MNSLE" titletext="Matrix notation for systems of linear equations"><h5 class="example">
<a knowl="./knowls/example.MNSLE.knowl"><span class="type">Example</span> <span class="acro">MNSLE</span></a> <span class="titletext">Matrix notation for systems of linear equations</span>
</h5></div>
<p>The matrix-vector product is a very natural computation.  We have motivated it by its connections with systems of equations, but here is another example.</p>
<div class="example" id="example-MBC" acro="MBC" titletext="Money's best cities"><h5 class="example">
<a knowl="./knowls/example.MBC.knowl"><span class="type">Example</span> <span class="acro">MBC</span></a> <span class="titletext">Money's best cities</span>
</h5></div>
<p>Later (much later) we will need the following theorem, which is really a technical lemma (see <a class="knowl" acro="LC" type="Proof Technique" title="Lemmas and Corollaries" knowl="./knowls/technique.LC.knowl">Proof Technique LC</a>).  Since we are in a position to prove it now, we will.  But you can safely skip it for the moment, if you promise to come back later to study the proof when the theorem is employed.  At that point you will also be able to understand the comments in the paragraph following the proof.</p>
<div class="theorem" id="theorem-EMMVP" acro="EMMVP" titletext="Equal Matrices and Matrix-Vector Products">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EMMVP</span><span class="titletext"> Equal Matrices and Matrix-Vector Products</span>
</h5>
<div class="statement"><p>Suppose that $A$ and $B$ are $m\times n$ matrices such that $A\vect{x}=B\vect{x}$ for every $\vect{x}\in\complex{n}$.  Then $A=B$.</p></div>
<div class="proof"><a knowl="./knowls/proof.EMMVP.knowl">Proof</a></div>
</div>
<p>You might notice from studying the proof that the hypotheses of this theorem could be “weakened” (i.e.   made less restrictive).   We need only suppose the equality of the matrix-vector products for just the standard unit vectors (<a class="knowl" acro="SUV" type="Definition" title="Standard Unit Vectors" knowl="./knowls/definition.SUV.knowl">Definition SUV</a>) or any other spanning set (<a class="knowl" acro="SSVS" type="Definition" title="Spanning Set of a Vector Space" knowl="./knowls/definition.SSVS.knowl">Definition SSVS</a>) of $\complex{n}$ (<a knowl="./knowls/exercise.LISS.T40.knowl">Exercise LISS.T40</a>).  However, in practice, when we apply this theorem the stronger hypothesis will be in effect so this version of the theorem will suffice for our purposes.  (If we changed the statement of the theorem to have the less restrictive hypothesis, then we would call the theorem “stronger.”)</p>
<div class="sage" id="sage-MVP" acro="MVP" titletext="Matrix-Vector Product"><h5 class="sage">
<a knowl="./knowls/sage.MVP.knowl"><span class="type">Sage</span> <span class="acro">MVP</span></a> <span class="titletext">Matrix-Vector Product</span>
</h5></div>
</div>
<div class="subsection" id="subsection-MM" acro="MM" titletext="Matrix Multiplication">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">MM</span> <span class="titletext">Matrix Multiplication</span>
</h4>
<p>We now define how to multiply two matrices together.  Stop for a minute and think about how you might define this new operation.</p>
<p>Many books would present this definition much earlier in the course.  However, we have taken great care to delay it as long as possible and to present as many ideas as practical based mostly on the notion of linear combinations.  Towards the conclusion of the course, or when you perhaps take a second course in linear algebra, you may be in a position to appreciate the reasons for this.  For now, understand that matrix multiplication is a central definition and perhaps you will appreciate its importance more by having saved it for later.</p>
<div class="definition" id="definition-MM" acro="MM" titletext="Matrix Multiplication">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">MM</span> <span class="titletext"> Matrix Multiplication</span>
</h5>
<p>Suppose $A$ is an $m\times n$ matrix and $\vectorlist{B}{p}$ are the columns of an $n\times p$ matrix $B$.  Then the <em class="term">matrix product</em> of $A$ with $B$ is the $m\times p$ matrix where column $i$ is the matrix-vector product $A\vect{B}_i$.  Symbolically,
\begin{equation*}
AB=A\matrixcolumns{B}{p}=\left[A\vect{B}_1|A\vect{B}_2|A\vect{B}_3|\ldots|A\vect{B}_p\right].
\end{equation*}
</p>
</div>
<div class="example" id="example-PTM" acro="PTM" titletext="Product of two matrices"><h5 class="example">
<a knowl="./knowls/example.PTM.knowl"><span class="type">Example</span> <span class="acro">PTM</span></a> <span class="titletext">Product of two matrices</span>
</h5></div>
<p>Is this the definition of matrix multiplication you expected?  Perhaps our previous operations for matrices caused you to think that we might multiply two matrices of the <em>same</em> size, <em>entry-by-entry</em>?  Notice that our current definition uses matrices of different sizes (though the number of columns in the first must equal the number of rows in the second), and the result is of a third size.  Notice too in the previous example that we cannot even consider the product $BA$, since the sizes of the two matrices in this order are not right.</p>
<p>But it gets weirder than that.  Many of your old ideas about “multiplication” will not apply to matrix multiplication, but some still will.  So make no assumptions, and do not do anything until you have a theorem that says you can.  Even if the sizes are right, matrix multiplication is not commutative — order matters.</p>
<div class="example" id="example-MMNC" acro="MMNC" titletext="Matrix multiplication is not commutative"><h5 class="example">
<a knowl="./knowls/example.MMNC.knowl"><span class="type">Example</span> <span class="acro">MMNC</span></a> <span class="titletext">Matrix multiplication is not commutative</span>
</h5></div>
</div>
<div class="subsection" id="subsection-MMEE" acro="MMEE" titletext="Matrix Multiplication, Entry-by-Entry">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">MMEE</span> <span class="titletext">Matrix Multiplication, Entry-by-Entry</span>
</h4>
<p>While certain “natural” properties of multiplication do not hold, many more do.  In the next subsection, we will state and prove the relevant theorems.  But first, we need a theorem that provides an alternate means of multiplying two matrices.  In many texts, this would be given as the <em>definition</em> of matrix multiplication.  We prefer to turn it around and have the following formula as a consequence of our definition.  It will prove useful for proofs of matrix equality, where we need to examine products of matrices, entry-by-entry.</p>
<div class="theorem" id="theorem-EMP" acro="EMP" titletext="Entries of Matrix Products">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EMP</span><span class="titletext"> Entries of Matrix Products</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then for $1\leq i\leq m$, $1\leq j\leq p$, the individual entries of $AB$ are given by
\begin{align*}
\matrixentry{AB}{ij}
&amp;=
\matrixentry{A}{i1}\matrixentry{B}{1j}+
\matrixentry{A}{i2}\matrixentry{B}{2j}+
\matrixentry{A}{i3}\matrixentry{B}{3j}+
\cdots+
\matrixentry{A}{in}\matrixentry{B}{nj}\\
&amp;=
\sum_{k=1}^{n}\matrixentry{A}{ik}\matrixentry{B}{kj}
\end{align*}
</p></div>
<div class="proof"><a knowl="./knowls/proof.EMP.knowl">Proof</a></div>
</div>
<div class="example" id="example-PTMEE" acro="PTMEE" titletext="Product of two matrices, entry-by-entry"><h5 class="example">
<a knowl="./knowls/example.PTMEE.knowl"><span class="type">Example</span> <span class="acro">PTMEE</span></a> <span class="titletext">Product of two matrices, entry-by-entry</span>
</h5></div>
<p><a class="knowl" acro="EMP" type="Theorem" title="Entries of Matrix Products" knowl="./knowls/theorem.EMP.knowl">Theorem EMP</a> is the way many people compute matrix products by hand.  It will also be very useful for the theorems we are going to prove shortly.  However, the definition (<a class="knowl" acro="MM" type="Definition" title="Matrix Multiplication" knowl="./knowls/definition.MM.knowl">Definition MM</a>) is frequently the most useful for its connections with deeper ideas like the null space and the upcoming column space.</p>
<div class="sage" id="sage-MM" acro="MM" titletext="Matrix Multiplication"><h5 class="sage">
<a knowl="./knowls/sage.MM.knowl"><span class="type">Sage</span> <span class="acro">MM</span></a> <span class="titletext">Matrix Multiplication</span>
</h5></div>
</div>
<div class="subsection" id="subsection-PMM" acro="PMM" titletext="Properties of Matrix Multiplication">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">PMM</span> <span class="titletext">Properties of Matrix Multiplication</span>
</h4>
<p>In this subsection, we collect properties of matrix multiplication and its interaction with
the zero matrix (<a class="knowl" acro="ZM" type="Definition" title="Zero Matrix" knowl="./knowls/definition.ZM.knowl">Definition ZM</a>),
the identity matrix (<a class="knowl" acro="IM" type="Definition" title="Identity Matrix" knowl="./knowls/definition.IM.knowl">Definition IM</a>),
matrix addition (<a class="knowl" acro="MA" type="Definition" title="Matrix Addition" knowl="./knowls/definition.MA.knowl">Definition MA</a>),
scalar matrix multiplication (<a class="knowl" acro="MSM" type="Definition" title="Matrix Scalar Multiplication" knowl="./knowls/definition.MSM.knowl">Definition MSM</a>),
the inner product (<a class="knowl" acro="IP" type="Definition" title="Inner Product" knowl="./knowls/definition.IP.knowl">Definition IP</a>),
conjugation (<a class="knowl" acro="MMCC" type="Theorem" title="Matrix Multiplication and Complex Conjugation" knowl="./knowls/theorem.MMCC.knowl">Theorem MMCC</a>),
and
the transpose (<a class="knowl" acro="TM" type="Definition" title="Transpose of a Matrix" knowl="./knowls/definition.TM.knowl">Definition TM</a>).
Whew!  Here we go.  These are great proofs to practice with, so try to concoct the proofs before reading them, they will get progressively more complicated as we go.</p>
<div class="theorem" id="theorem-MMZM" acro="MMZM" titletext="Matrix Multiplication and the Zero Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMZM</span><span class="titletext"> Matrix Multiplication and the Zero Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix.  Then
<ol>
<li> $A\zeromatrix_{n\times p}=\zeromatrix_{m\times p}$
</li>
<li> $\zeromatrix_{p\times m}A=\zeromatrix_{p\times n}$
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.MMZM.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-MMIM" acro="MMIM" titletext="Matrix Multiplication and Identity Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMIM</span><span class="titletext"> Matrix Multiplication and Identity Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix.  Then
<ol>
<li> $AI_n=A$
</li>
<li> $I_mA=A$
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.MMIM.knowl">Proof</a></div>
</div>
<p>It is this theorem that gives the identity matrix its name.  It is a matrix that behaves with matrix multiplication like the scalar 1 does with scalar multiplication.  To multiply by the identity matrix is to have no effect on the other matrix.</p>
<div class="theorem" id="theorem-MMDAA" acro="MMDAA" titletext="Matrix Multiplication Distributes Across Addition">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMDAA</span><span class="titletext"> Matrix Multiplication Distributes Across Addition</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ and $C$ are $n\times p$ matrices and $D$ is a $p\times s$ matrix.    Then
<ol>
<li> $A(B+C)=AB+AC$
</li>
<li> $(B+C)D=BD+CD$
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.MMDAA.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-MMSMM" acro="MMSMM" titletext="Matrix Multiplication and Scalar Matrix Multiplication">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMSMM</span><span class="titletext"> Matrix Multiplication and Scalar Matrix Multiplication</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Let $\alpha$ be a scalar.  Then $\alpha(AB)=(\alpha A)B=A(\alpha B)$.</p></div>
<div class="proof"><a knowl="./knowls/proof.MMSMM.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-MMA" acro="MMA" titletext="Matrix Multiplication is Associative">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMA</span><span class="titletext"> Matrix Multiplication is Associative</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix, $B$ is an $n\times p$ matrix and $D$ is a $p\times s$ matrix.  Then  $A(BD)=(AB)D$.</p></div>
<div class="proof"><a knowl="./knowls/proof.MMA.knowl">Proof</a></div>
</div>
<p>Since <a class="knowl" acro="MMA" type="Theorem" title="Matrix Multiplication is Associative" knowl="./knowls/theorem.MMA.knowl">Theorem MMA</a> says matrix multipication is associative, it means we do not have to be careful about the order in which we perform matrix multiplication, nor how we parenthesize an expression with just several matrices multiplied togther.  So this is where we draw the line on explaining every last detail in a proof.  We will frequently add, remove, or rearrange parentheses with no comment.  Indeed, I only see about a dozen places where <a class="knowl" acro="MMA" type="Theorem" title="Matrix Multiplication is Associative" knowl="./knowls/theorem.MMA.knowl">Theorem MMA</a> is cited in a proof.  You could try to count how many times we <em>avoid</em> making a reference to this theorem.</p>
<p>The statement of our next theorem is technically inaccurate.  If we upgrade the vectors $\vect{u},\,\vect{v}$ to matrices with a single column, then the expression $\transpose{\conjugate{\vect{u}}}\vect{v}$ is a $1\times 1$ matrix, though we will treat this small matrix as if it was simply the scalar quantity in its lone entry.  When we apply <a class="knowl" acro="MMIP" type="Theorem" title="Matrix Multiplication and Inner Products" knowl="./knowls/theorem.MMIP.knowl">Theorem MMIP</a> there should not be any confusion.  Notice that if we treat a column vector as a matrix with a single column, then we can also construct the adjoint of a vector, though we will not make this a common practice.</p>
<div class="theorem" id="theorem-MMIP" acro="MMIP" titletext="Matrix Multiplication and Inner Products">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMIP</span><span class="titletext"> Matrix Multiplication and Inner Products</span>
</h5>
<div class="statement"><p>If we consider the vectors $\vect{u},\,\vect{v}\in\complex{m}$ as $m\times 1$ matrices then
\begin{align*}
\innerproduct{\vect{u}}{\vect{v}}
&amp;=\transpose{\conjugate{\vect{u}}}\vect{v}
=\adjoint{\vect{u}}\vect{v}
\end{align*}

</p></div>
<div class="proof"><a knowl="./knowls/proof.MMIP.knowl">Proof</a></div>
</div>
<div class="theorem" id="theorem-MMCC" acro="MMCC" titletext="Matrix Multiplication and Complex Conjugation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMCC</span><span class="titletext"> Matrix Multiplication and Complex Conjugation</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\conjugate{AB}=\conjugate{A}\,\conjugate{B}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.MMCC.knowl">Proof</a></div>
</div>
<p>Another theorem in this style, and it is a good one.  If you have been practicing with the previous proofs you should be able to do this one yourself.</p>
<div class="theorem" id="theorem-MMT" acro="MMT" titletext="Matrix Multiplication and Transposes">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMT</span><span class="titletext"> Matrix Multiplication and Transposes</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\transpose{(AB)}=\transpose{B}\transpose{A}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.MMT.knowl">Proof</a></div>
</div>
<p>This theorem seems odd at first glance, since we have to switch the order of $A$ and $B$.  But if we simply consider the sizes of the matrices involved, we can see that the switch is necessary for this reason alone.  That the individual entries of the products then come along to be equal is a bonus.</p>
<p>As the adjoint of a matrix is a composition of a conjugate and a transpose, its interaction with matrix multiplication is similar to that of a transpose.  Here is the last of our long list of basic properties of matrix multiplication.</p>
<div class="theorem" id="theorem-MMAD" acro="MMAD" titletext="Matrix Multiplication and Adjoints">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">MMAD</span><span class="titletext"> Matrix Multiplication and Adjoints</span>
</h5>
<div class="statement"><p>Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix.  Then $\adjoint{(AB)}=\adjoint{B}\adjoint{A}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.MMAD.knowl">Proof</a></div>
</div>
<p>Notice how none of these proofs above relied on writing out huge general matrices with lots of ellipses (“…”) and trying to formulate the equalities a whole matrix at a time.  This messy business is a “proof technique” to be avoided at all costs.  Notice too how the proof of <a class="knowl" acro="MMAD" type="Theorem" title="Matrix Multiplication and Adjoints" knowl="./knowls/theorem.MMAD.knowl">Theorem MMAD</a> does not use an entry-by-entry approach, but simply builds on previous results about matrix multiplication's interaction with conjugation and transposes.</p>
<p>These theorems, along with <a class="knowl" acro="VSPM" type="Theorem" title="Vector Space Properties of Matrices" knowl="./knowls/theorem.VSPM.knowl">Theorem VSPM</a> and the other results in <a href="section-MO.html" title="Matrix Operations">Section MO</a>, give you the “rules” for how matrices interact with the various operations we have defined on matrices (addition, scalar multiplication, matrix multiplication, conjugation, transposes and adjoints).  Use them and use them often.  But do not try to do anything with a matrix that you do not have a rule for.  Together, we would informally call all these operations, and the attendant theorems, “the algebra of matrices.”  Notice, too, that every column vector is just a $n\times 1$ matrix, so these theorems apply to column vectors also.  Finally, these results, taken as a whole, may make us feel that the definition of matrix multiplication is not so unnatural.</p>
<div class="sage" id="sage-PMM" acro="PMM" titletext="Properties of Matrix Multiplication"><h5 class="sage">
<a knowl="./knowls/sage.PMM.knowl"><span class="type">Sage</span> <span class="acro">PMM</span></a> <span class="titletext">Properties of Matrix Multiplication</span>
</h5></div>
</div>
<div class="subsection" id="subsection-HM" acro="HM" titletext="Hermitian Matrices">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">HM</span> <span class="titletext">Hermitian Matrices</span>
</h4>
<p>The adjoint of a matrix has a basic property when employed in a matrix-vector product as part of an inner product.  At this point, you could even use the following result as a motivation for the definition of an adjoint.</p>
<div class="theorem" id="theorem-AIP" acro="AIP" titletext="Adjoint and Inner Product">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">AIP</span><span class="titletext"> Adjoint and Inner Product</span>
</h5>
<div class="statement"><p>Suppose that $A$ is an $m\times n$ matrix and $\vect{x}\in\complex{n}$, $\vect{y}\in\complex{m}$.  Then $\innerproduct{A\vect{x}}{\vect{y}}=\innerproduct{\vect{x}}{\adjoint{A}\vect{y}}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.AIP.knowl">Proof</a></div>
</div>
<p>Sometimes a matrix is equal to its adjoint (<a class="knowl" acro="A" type="Definition" title="Adjoint" knowl="./knowls/definition.A.knowl">Definition A</a>), and these matrices have interesting properties.  One of the most common situations where this occurs is when a matrix has only real number entries.  Then we are simply talking about symmetric matrices (<a class="knowl" acro="SYM" type="Definition" title="Symmetric Matrix" knowl="./knowls/definition.SYM.knowl">Definition SYM</a>), so you can view this as a generalization of a symmetric matrix.</p>
<div class="definition" id="definition-HM" acro="HM" titletext="Hermitian Matrix">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">HM</span> <span class="titletext"> Hermitian Matrix</span>
</h5>
<p>The square matrix $A$ is <em class="term">Hermitian</em> (or <em class="term">self-adjoint</em>) if $A=\adjoint{A}$.</p>
</div>
<p>Again, the set of real matrices that are Hermitian is exactly the set of symmetric matrices.  In <a href="section-PEE.html" title="Properties of Eigenvalues and Eigenvectors">Section PEE</a> we will uncover some amazing properties of Hermitian matrices, so when you get there, run back here to remind yourself of this definition.  Further properties will also appear in <a href="section-OD.html" title="Orthonormal Diagonalization">Section OD</a>.  Right now we prove a fundamental result about Hermitian matrices, matrix vector products and inner products.  As a characterization, this could be employed as a definition of a Hermitian matrix and some authors take this approach.</p>
<div class="theorem" id="theorem-HMIP" acro="HMIP" titletext="Hermitian Matrices and Inner Products">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">HMIP</span><span class="titletext"> Hermitian Matrices and Inner Products</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix of size $n$.  Then $A$ is Hermitian if and only if $\innerproduct{A\vect{x}}{\vect{y}}=\innerproduct{\vect{x}}{A\vect{y}}$ for all $\vect{x},\,\vect{y}\in\complex{n}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.HMIP.knowl">Proof</a></div>
</div>
<p>So, informally, Hermitian matrices are those that can be tossed around from one side of an inner product to the other with reckless abandon.  We will see later what this buys us.</p>
</div>
<div class="readingquestions" id="readingquestions-MM" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.MM.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-MM" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.MM.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
