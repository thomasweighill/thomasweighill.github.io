<div class="annotatedacronyms" id="annotatedacronyms-E" acro="E" titletext="">
<h5 class="annotatedacronyms">
<span class="type">Annotated Acronyms for Chapter</span> <span class="acro">E</span>
</h5>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="EMRCP" type="Theorem" title="Eigenvalues of a Matrix are Roots of Characteristic Polynomials" knowl="./knowls/theorem.EMRCP.knowl">Theorem EMRCP</a></h5></div>
<p>
Much of what we know about eigenvalues can be traced to analysis of the characteristic polynomial.  When we first defined eigenvalues, you might have wondered if they were scarce, or abundant.  The characteristic polynomial allows us to answer a question like this with a result like <a class="knowl" acro="NEM" type="Theorem" title="Number of Eigenvalues of a Matrix" knowl="./knowls/theorem.NEM.knowl">Theorem NEM</a> which tells us there are always a few eigenvalues, but never too many.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="EMNS" type="Theorem" title="Eigenspace of a Matrix is a Null Space" knowl="./knowls/theorem.EMNS.knowl">Theorem EMNS</a></h5></div>
<p>
If <a class="knowl" acro="EMRCP" type="Theorem" title="Eigenvalues of a Matrix are Roots of Characteristic Polynomials" knowl="./knowls/theorem.EMRCP.knowl">Theorem EMRCP</a> allows us to learn about eigenvalues through what we know about roots of polynomials, then <a class="knowl" acro="EMNS" type="Theorem" title="Eigenspace of a Matrix is a Null Space" knowl="./knowls/theorem.EMNS.knowl">Theorem EMNS</a> allows us to learn about eigenvectors, and eigenspaces, from what we already know about null spaces.  These two theorems, along with <a class="knowl" acro="EEM" type="Definition" title="Eigenvalues and Eigenvectors of a Matrix" knowl="./knowls/definition.EEM.knowl">Definition EEM</a>, provide the starting points for discerning the properties of eigenvalues and eigenvectors (to say nothing of actually computing them).
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="HMRE" type="Theorem" title="Hermitian Matrices have Real Eigenvalues" knowl="./knowls/theorem.HMRE.knowl">Theorem HMRE</a></h5></div>
<p>
As we have remarked before, we choose to include all of the complex numbers in our set of allowed scalars, whereas many introductory texts restrict their attention to just the real numbers.  Here is one of the payoffs to this approach.  Begin with a matrix, possibly containing complex entries, and require the matrix to be Hermitian (<a class="knowl" acro="HM" type="Definition" title="Hermitian Matrix" knowl="./knowls/definition.HM.knowl">Definition HM</a>).  In the case of only real entries, this boils down to just requiring the matrix to be symmetric (<a class="knowl" acro="SYM" type="Definition" title="Symmetric Matrix" knowl="./knowls/definition.SYM.knowl">Definition SYM</a>).  Generally, the roots of a characteristic polynomial, even with all real coefficients, can have complex numbers as roots.  But for a Hermitian matrix, all of the eigenvalues are real numbers!  When somebody tells you mathematics can be beautiful, this is an example of what they are talking about.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="DC" type="Theorem" title="Diagonalization Characterization" knowl="./knowls/theorem.DC.knowl">Theorem DC</a></h5></div>
<p>
Diagonalizing a matrix, or the question of if a matrix is diagonalizable, could be viewed as one of a handful of central questions in linear algebra.  Here we have an unequivocal answer to the question of “if,” along with a proof containing a construction for the diagonalization.  So this theorem is of theoretical and computational interest.  This topic will be important again in <a href="chapter-R.html" title="Representations">Chapter R</a>.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="DMFE" type="Theorem" title="Diagonalizable Matrices have Full Eigenspaces" knowl="./knowls/theorem.DMFE.knowl">Theorem DMFE</a></h5></div>
<p>
Another unequivocal answer to the question of if a matrix is diagonalizable, with perhaps a simpler condition to test.  The proof also tells us how to construct the necessary set of $n$ linearly independent eigenvectors — just round up bases for each eigenspace and join them together.  No need to test the linear independence of the combined set.
</p>
</div>
