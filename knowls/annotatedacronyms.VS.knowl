<div class="annotatedacronyms" id="annotatedacronyms-VS" acro="VS" titletext="">
<h5 class="annotatedacronyms">
<span class="type">Annotated Acronyms for Chapter</span> <span class="acro">VS</span>
</h5>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="VS" type="Definition" title="Vector Space" knowl="./knowls/definition.VS.knowl">Definition VS</a></h5></div>
<p>
The most fundamental object in linear algebra is a vector space.  Or else the most fundamental object is a vector, and a vector space is important because it is a collection of vectors.  Either way, <a class="knowl" acro="VS" type="Definition" title="Vector Space" knowl="./knowls/definition.VS.knowl">Definition VS</a> is critical.  All of our remaining theorems that assume we are working with a vector space can trace their lineage back to this definition.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="TSS" type="Theorem" title="Testing Subsets for Subspaces" knowl="./knowls/theorem.TSS.knowl">Theorem TSS</a></h5></div>
<p>
Checking all ten properties of a vector space (<a class="knowl" acro="VS" type="Definition" title="Vector Space" knowl="./knowls/definition.VS.knowl">Definition VS</a>) can get tedious.  But if you have a subset of a <em>known</em> vector space, then <a class="knowl" acro="TSS" type="Theorem" title="Testing Subsets for Subspaces" knowl="./knowls/theorem.TSS.knowl">Theorem TSS</a> considerably shortens the verification.  Also, proofs of closure (the last two conditions in <a class="knowl" acro="TSS" type="Theorem" title="Testing Subsets for Subspaces" knowl="./knowls/theorem.TSS.knowl">Theorem TSS</a>) are a good way to practice a common style of proof.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="VRRB" type="Theorem" title="Vector Representation Relative to a Basis" knowl="./knowls/theorem.VRRB.knowl">Theorem VRRB</a></h5></div>
<p>
The proof of uniqueness in this theorem is a very typical employment of the hypothesis of linear independence.  But that is not why we mention it here.  This theorem is critical to our first section about representations, <a href="section-VR.html" title="Vector Representations">Section VR</a>, via <a class="knowl" acro="VR" type="Definition" title="Vector Representation" knowl="./knowls/definition.VR.knowl">Definition VR</a>.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="CNMB" type="Theorem" title="Columns of Nonsingular Matrix are a Basis" knowl="./knowls/theorem.CNMB.knowl">Theorem CNMB</a></h5></div>
<p>
Having just defined a basis (<a class="knowl" acro="B" type="Definition" title="Basis" knowl="./knowls/definition.B.knowl">Definition B</a>) we discover that the columns of a nonsingular matrix form a basis of $\complex{m}$.  Much of what we know about nonsingular matrices is either contained in this statement, or much more evident because of it.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="SSLD" type="Theorem" title="Spanning Sets and Linear Dependence" knowl="./knowls/theorem.SSLD.knowl">Theorem SSLD</a></h5></div>
<p>
This theorem is a key juncture in our development of linear algebra.  You have probably already realized how useful <a class="knowl" acro="G" type="Theorem" title="Goldilocks" knowl="./knowls/theorem.G.knowl">Theorem G</a> is.  All four parts of <a class="knowl" acro="G" type="Theorem" title="Goldilocks" knowl="./knowls/theorem.G.knowl">Theorem G</a> have proofs that finish with an application of <a class="knowl" acro="SSLD" type="Theorem" title="Spanning Sets and Linear Dependence" knowl="./knowls/theorem.SSLD.knowl">Theorem SSLD</a>.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="RPNC" type="Theorem" title="Rank Plus Nullity is Columns" knowl="./knowls/theorem.RPNC.knowl">Theorem RPNC</a></h5></div>
<p>
This simple relationship between the rank, nullity and number of columns of a matrix might be surprising.  But in simplicity comes power, as this theorem can be very useful.  It will be generalized in the very last theorem of <a href="chapter-LT.html" title="Linear Transformations">Chapter LT</a>, <a class="knowl" acro="RPNDD" type="Theorem" title="Rank Plus Nullity is Domain Dimension" knowl="./knowls/theorem.RPNDD.knowl">Theorem RPNDD</a>.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="G" type="Theorem" title="Goldilocks" knowl="./knowls/theorem.G.knowl">Theorem G</a></h5></div>
<p>
A whimsical title, but the intent is to make sure you do not miss this one.  Much of the interaction between bases, dimension, linear independence and spanning is captured in this theorem.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="RMRT" type="Theorem" title="Rank of a Matrix is the Rank of the Transpose" knowl="./knowls/theorem.RMRT.knowl">Theorem RMRT</a></h5></div>
<p>
This one is a real surprise.  Why should a matrix, and its transpose, both row-reduce to the same number of nonzero rows?
</p>
</div>
