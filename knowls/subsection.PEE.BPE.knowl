<div class="subsection" id="subsection-BPE" acro="BPE" titletext="Basic Properties of Eigenvalues">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">BPE</span> <span class="titletext">Basic Properties of Eigenvalues</span>
</h4>
<div class="theorem" id="theorem-EDELI" acro="EDELI" titletext="Eigenvectors with Distinct Eigenvalues are Linearly Independent">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EDELI</span><span class="titletext"> Eigenvectors with Distinct Eigenvalues are Linearly Independent</span>
</h5>
<div class="statement"><p>Suppose that $A$ is an $n\times n$ square matrix and $S=\set{\vectorlist{x}{p}}$ is a set of eigenvectors with eigenvalues $\scalarlist{\lambda}{p}$ such that $\lambda_i\neq\lambda_j$ whenever $i\neq j$.  Then $S$ is a linearly independent set.</p></div>
<div class="proof"><a knowl="./knowls/proof.EDELI.knowl">Proof</a></div>
</div>
<p>There is a simple connection between the eigenvalues of a matrix and whether or not the matrix is nonsingular.</p>
<div class="theorem" id="theorem-SMZE" acro="SMZE" titletext="Singular Matrices have Zero Eigenvalues">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">SMZE</span><span class="titletext"> Singular Matrices have Zero Eigenvalues</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix.  Then $A$ is singular if and only if $\lambda=0$ is an eigenvalue of $A$.</p></div>
<div class="proof"><a knowl="./knowls/proof.SMZE.knowl">Proof</a></div>
</div>
<p>With an equivalence about singular matrices we can update our list of equivalences about nonsingular matrices.</p>
<div class="theorem" id="theorem-NME8" acro="NME8" titletext="Nonsingular Matrix Equivalences, Round 8">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">NME8</span><span class="titletext"> Nonsingular Matrix Equivalences, Round 8</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix of size $n$.  The following are equivalent.
<ol>
<li> $A$ is nonsingular.
</li>
<li> $A$ row-reduces to the identity matrix.
</li>
<li> The null space of $A$ contains only the zero vector, $\nsp{A}=\set{\zerovector}$.
</li>
<li> The linear system $\linearsystem{A}{\vect{b}}$ has a unique solution for every possible choice of $\vect{b}$.
</li>
<li> The columns of $A$ are a linearly independent set.
</li>
<li> $A$ is invertible.
</li>
<li> The column space of $A$ is $\complex{n}$, $\csp{A}=\complex{n}$.
</li>
<li> The columns of $A$ are a basis for $\complex{n}$.
</li>
<li> The rank of $A$ is $n$, $\rank{A}=n$.
</li>
<li> The nullity of $A$ is zero, $\nullity{A}=0$.
</li>
<li> The determinant of $A$ is nonzero, $\detname{A}\neq 0$.
</li>
<li> $\lambda=0$ is not an eigenvalue of $A$.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.NME8.knowl">Proof</a></div>
</div>
<div class="sage" id="sage-NME8" acro="NME8" titletext="Nonsingular Matrix Equivalences, Round 8"><h5 class="sage">
<a knowl="./knowls/sage.NME8.knowl"><span class="type">Sage</span> <span class="acro">NME8</span></a> <span class="titletext">Nonsingular Matrix Equivalences, Round 8</span>
</h5></div>
<p>Certain changes to a matrix change its eigenvalues in a predictable way.</p>
<div class="theorem" id="theorem-ESMM" acro="ESMM" titletext="Eigenvalues of a Scalar Multiple of a Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ESMM</span><span class="titletext"> Eigenvalues of a Scalar Multiple of a Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix and $\lambda$ is an eigenvalue of $A$.  Then $\alpha\lambda$ is an eigenvalue of $\alpha A$.</p></div>
<div class="proof"><a knowl="./knowls/proof.ESMM.knowl">Proof</a></div>
</div>
<p>Unfortunately, there are not parallel theorems about the sum or product of arbitrary matrices.  But we can prove a similar result for powers of a matrix.</p>
<div class="theorem" id="theorem-EOMP" acro="EOMP" titletext="Eigenvalues Of Matrix Powers">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EOMP</span><span class="titletext"> Eigenvalues Of Matrix Powers</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix, $\lambda$ is an eigenvalue of $A$, and $s\geq 0$ is an integer.  Then $\lambda^s$ is an eigenvalue of $A^s$.</p></div>
<div class="proof"><a knowl="./knowls/proof.EOMP.knowl">Proof</a></div>
</div>
<p>While we cannot prove that the sum of two arbitrary matrices behaves in any reasonable way with regard to eigenvalues, we can work with the sum of dissimilar powers of the <em>same</em> matrix.  We have already seen two connections between eigenvalues and polynomials, in the proof of <a class="knowl" acro="EMHE" type="Theorem" title="Every Matrix Has an Eigenvalue" knowl="./knowls/theorem.EMHE.knowl">Theorem EMHE</a> and the characteristic polynomial (<a class="knowl" acro="CP" type="Definition" title="Characteristic Polynomial" knowl="./knowls/definition.CP.knowl">Definition CP</a>).  Our next theorem strengthens this connection.</p>
<div class="theorem" id="theorem-EPM" acro="EPM" titletext="Eigenvalues of the Polynomial of a Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EPM</span><span class="titletext"> Eigenvalues of the Polynomial of a Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix and $\lambda$ is an eigenvalue of $A$.  Let $q(x)$ be a polynomial in the variable $x$.  Then $q(\lambda)$ is an eigenvalue of the matrix $q(A)$.</p></div>
<div class="proof"><a knowl="./knowls/proof.EPM.knowl">Proof</a></div>
</div>
<div class="example" id="example-BDE" acro="BDE" titletext="Building desired eigenvalues"><h5 class="example">
<a knowl="./knowls/example.BDE.knowl"><span class="type">Example</span> <span class="acro">BDE</span></a> <span class="titletext">Building desired eigenvalues</span>
</h5></div>
<p>Inverses and transposes also behave predictably with regard to their eigenvalues.</p>
<div class="theorem" id="theorem-EIM" acro="EIM" titletext="Eigenvalues of the Inverse of a Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">EIM</span><span class="titletext"> Eigenvalues of the Inverse of a Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square nonsingular matrix and $\lambda$ is an eigenvalue of $A$.  Then $\lambda^{-1}$ is an eigenvalue of the matrix $\inverse{A}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.EIM.knowl">Proof</a></div>
</div>
<p>The proofs of the theorems above have a similar style to them.  They all begin by grabbing an eigenvalue-eigenvector pair and adjusting it in some way to reach the desired conclusion.  You should add this to your toolkit as a general approach to proving theorems about eigenvalues.</p>
<p>So far we have been able to reserve the characteristic polynomial for strictly computational purposes.  However, sometimes a theorem about eigenvalues can be proved easily by employing the characteristic polynomial (rather than using an eigenvalue-eigenvector pair).  The next theorem is an example of this.</p>
<div class="theorem" id="theorem-ETM" acro="ETM" titletext="Eigenvalues of the Transpose of a Matrix">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ETM</span><span class="titletext"> Eigenvalues of the Transpose of a Matrix</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix and $\lambda$ is an eigenvalue of $A$.  Then $\lambda$ is an eigenvalue of the matrix $\transpose{A}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.ETM.knowl">Proof</a></div>
</div>
<p>If a matrix has only real entries, then the computation of the characteristic polynomial (<a class="knowl" acro="CP" type="Definition" title="Characteristic Polynomial" knowl="./knowls/definition.CP.knowl">Definition CP</a>) will result in a polynomial with coefficients that are real numbers.  Complex numbers could result as roots of this polynomial, but they are roots of quadratic factors with real coefficients, and as such, come in conjugate pairs.  The next theorem proves this, and a bit more, without mentioning the characteristic polynomial.</p>
<div class="theorem" id="theorem-ERMCP" acro="ERMCP" titletext="Eigenvalues of Real Matrices come in Conjugate Pairs">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ERMCP</span><span class="titletext"> Eigenvalues of Real Matrices come in Conjugate Pairs</span>
</h5>
<div class="statement"><p>Suppose $A$ is a square matrix with real entries and $\vect{x}$ is an eigenvector of $A$ for the eigenvalue $\lambda$.  Then $\conjugate{\vect{x}}$ is an eigenvector of $A$ for the eigenvalue $\conjugate{\lambda}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.ERMCP.knowl">Proof</a></div>
</div>
<p>This phenomenon is amply illustrated in <a class="knowl" acro="CEMS6" type="Example" title="Complex eigenvalues, matrix of size 6" knowl="./knowls/example.CEMS6.knowl">Example CEMS6</a>, where the four complex eigenvalues come in two pairs, and the two basis vectors of the eigenspaces are complex conjugates of each other.  <a class="knowl" acro="ERMCP" type="Theorem" title="Eigenvalues of Real Matrices come in Conjugate Pairs" knowl="./knowls/theorem.ERMCP.knowl">Theorem ERMCP</a> can be a time-saver for computing eigenvalues and eigenvectors of real matrices with complex eigenvalues, since the conjugate eigenvalue and eigenspace can be inferred from the theorem rather than computed.</p>
</div><div class="context"><a href="http://linear.pugetsound.edu/html/section-PEE.html#subsection-BPE" class="context" title="Section PEE">(in context)</a></div>
