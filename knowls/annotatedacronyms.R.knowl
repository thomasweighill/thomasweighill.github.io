<div class="annotatedacronyms" id="annotatedacronyms-R" acro="R" titletext="">
<h5 class="annotatedacronyms">
<span class="type">Annotated Acronyms for Chapter</span> <span class="acro">R</span>
</h5>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="VR" type="Definition" title="Vector Representation" knowl="./knowls/definition.VR.knowl">Definition VR</a></h5></div>
<p>
Matrix representations build on vector representations, so this is the definition that gets us started.  A representation depends on the choice of a single basis for the vector space.  <a class="knowl" acro="VRRB" type="Theorem" title="Vector Representation Relative to a Basis" knowl="./knowls/theorem.VRRB.knowl">Theorem VRRB</a> is what tells us this idea might be useful.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="VRILT" type="Theorem" title="Vector Representation is an Invertible Linear Transformation" knowl="./knowls/theorem.VRILT.knowl">Theorem VRILT</a></h5></div>
<p>
As an invertible linear transformation, vector representation allows us to translate, back and forth, between abstract vector spaces ($V$) and concrete vector spaces ($\complex{n}$).  This is key to all our notions of representations in this chapter.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="CFDVS" type="Theorem" title="Characterization of Finite Dimensional Vector Spaces" knowl="./knowls/theorem.CFDVS.knowl">Theorem CFDVS</a></h5></div>
<p>
Every vector space with finite dimension “looks like” a vector space of column vectors.  Vector representation is the isomorphism that establishes that these vector spaces are isomorphic.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="MR" type="Definition" title="Matrix Representation" knowl="./knowls/definition.MR.knowl">Definition MR</a></h5></div>
<p>
Building on the definition of a vector representation, we define a representation of a linear transformation, determined by a choice of two bases, one for the domain and one for the codomain.  Notice that vectors are represented by columnar lists of scalars, while linear transformations are represented by rectangular tables of scalars.  Building a matrix representation is as important a skill as row-reducing a matrix.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="FTMR" type="Theorem" title="Fundamental Theorem of Matrix Representation" knowl="./knowls/theorem.FTMR.knowl">Theorem FTMR</a></h5></div>
<p>
<a class="knowl" acro="MR" type="Definition" title="Matrix Representation" knowl="./knowls/definition.MR.knowl">Definition MR</a> is not really very interesting until we have this theorem.  The second form tells us that we can compute outputs of linear transformations via matrix multiplication, along with some bookkeeping for vector representations.  Searching forward through the text on “FTMR” is an interesting exercise.  You will find reference to this result buried inside many key proofs at critical points, and it also appears in numerous examples and solutions to exercises.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="MRCLT" type="Theorem" title="Matrix Representation of a Composition of Linear Transformations" knowl="./knowls/theorem.MRCLT.knowl">Theorem MRCLT</a></h5></div>
<p>
Turns out that matrix multiplication is really a very natural operation, it is just the chaining together (composition) of functions (linear transformations).  Beautiful.  Even if you do not try to work the problem, study <a knowl="./knowls/solution.MR.T80.knowl">Solution MR.T80</a> for more insight.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="KNSI" type="Theorem" title="Kernel and Null Space Isomorphism" knowl="./knowls/theorem.KNSI.knowl">Theorem KNSI</a></h5></div>
<p>
Kernels “are” null spaces.  For this reason you will see these terms used interchangeably.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="RCSI" type="Theorem" title="Range and Column Space Isomorphism" knowl="./knowls/theorem.RCSI.knowl">Theorem RCSI</a></h5></div>
<p>
Ranges “are” column spaces.  For this reason you will see these terms used interchangeably.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="IMR" type="Theorem" title="Invertible Matrix Representations" knowl="./knowls/theorem.IMR.knowl">Theorem IMR</a></h5></div>
<p>
Invertible linear transformations are represented by invertible (nonsingular) matrices.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="NME9" type="Theorem" title="Nonsingular Matrix Equivalences, Round 9" knowl="./knowls/theorem.NME9.knowl">Theorem NME9</a></h5></div>
<p>
The NMEx series has always been important, but we have held off saying so until now.  This is the end of the line for this one, so it is a good time to contemplate all that it means.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="SCB" type="Theorem" title="Similarity and Change of Basis" knowl="./knowls/theorem.SCB.knowl">Theorem SCB</a></h5></div>
<p>
Diagonalization back in <a href="section-SD.html" title="Similarity and Diagonalization">Section SD</a> was really a change of basis to achieve a diagonal matrix repesentation.  Maybe we should be highlighting the more general <a class="knowl" acro="MRCB" type="Theorem" title="Matrix Representation and Change of Basis" knowl="./knowls/theorem.MRCB.knowl">Theorem MRCB</a> here, but its overly technical description just is not as appealing.  However, it will be important in some of the matrix decompositions you will encounter in a future course in linear algebra.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="EER" type="Theorem" title="Eigenvalues, Eigenvectors, Representations" knowl="./knowls/theorem.EER.knowl">Theorem EER</a></h5></div>
<p>
This theorem, with the companion definition, <a class="knowl" acro="EELT" type="Definition" title="Eigenvalue and Eigenvector of a Linear Transformation" knowl="./knowls/definition.EELT.knowl">Definition EELT</a>, tells us that eigenvalues, and eigenvectors, are fundamentally a characteristic of linear transformations (not matrices).  If you study matrix decompositions in a future course in linear algebra you will come to appreciate that almost all of a matrix's secrets can be unlocked with knowledge of the eigenvalues and eigenvectors.
</p>
<div class="annoacro"><h5 class="annoacro"><a class="knowl" acro="OD" type="Theorem" title="Orthonormal Diagonalization" knowl="./knowls/theorem.OD.knowl">Theorem OD</a></h5></div>
<p>
Can you imagine anything nicer than an orthonormal diagonalization?  A basis of pairwise orthogonal, unit norm, eigenvectors that provide a diagonal representation for a matrix?  Here we learn just when this can happen — precisely when a matrix is normal, which is a disarmingly simple property to define.
</p>
</div>
