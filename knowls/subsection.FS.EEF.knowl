<div class="subsection" id="subsection-EEF" acro="EEF" titletext="Extended Echelon Form">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">EEF</span> <span class="titletext">Extended Echelon Form</span>
</h4>
<p>The final matrix that we row-reduced in <a class="knowl" acro="CSANS" type="Example" title="Column space as null space" knowl="./knowls/example.CSANS.knowl">Example CSANS</a> should look familiar in most respects to the procedure we used to compute the inverse of a nonsingular matrix, <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a>.  We will now generalize that procedure to matrices that are not necessarily nonsingular, or even square.  First a definition.</p>
<div class="definition" id="definition-EEF" acro="EEF" titletext="Extended Echelon Form">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">EEF</span> <span class="titletext"> Extended Echelon Form</span>
</h5>
<p>Suppose $A$ is an $m\times n$ matrix.  Extend $A$ on its right side with the addition of an $m\times m$ identity matrix to form an $m\times (n + m)$ matrix $M$.  Use row operations to bring $M$ to reduced row-echelon form and call the result $N$.  $N$ is the <em class="term">extended reduced row-echelon form</em> of $A$, and we will standardize on names for five submatrices ($B$, $C$, $J$, $K$, $L$) of $N$.</p>
<p>Let $B$ denote the $m\times n$ matrix formed from the first $n$ columns of $N$ and let $J$ denote the $m\times m$ matrix formed from the last $m$ columns of $N$.  Suppose that $B$ has $r$ nonzero rows.  Further partition $N$ by letting $C$ denote the $r\times n$ matrix formed from all of the nonzero rows of $B$.  Let $K$ be the $r\times m$ matrix formed from the first $r$ rows of $J$, while $L$ will be the $(m-r)\times m$ matrix formed from the bottom $m-r$ rows of $J$.  Pictorially,
\begin{equation*}
M=[A\vert I_m]
\rref
N=[B\vert J]
=
\left[\begin{array}{c|c}C&amp;K\\\hline0&amp;L\end{array}\right]
\end{equation*}
</p>
</div>
<div class="example" id="example-SEEF" acro="SEEF" titletext="Submatrices of extended echelon form"><h5 class="example">
<a knowl="./knowls/example.SEEF.knowl"><span class="type">Example</span> <span class="acro">SEEF</span></a> <span class="titletext">Submatrices of extended echelon form</span>
</h5></div>
<div class="theorem" id="theorem-PEEF" acro="PEEF" titletext="Properties of Extended Echelon Form">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">PEEF</span><span class="titletext"> Properties of Extended Echelon Form</span>
</h5>
<div class="statement"><p>Suppose that $A$ is an $m\times n$ matrix and that $N$ is its extended echelon form.  Then
<ol>
<li> $J$ is nonsingular.
</li>
<li> $B=JA$.
</li>
<li> If $\vect{x}\in\complex{n}$ and $\vect{y}\in\complex{m}$, then $A\vect{x}=\vect{y}$ if and only if $B\vect{x}=J\vect{y}$.
</li>
<li> $C$ is in reduced row-echelon form, has no zero rows and has $r$ pivot columns.
</li>
<li> $L$ is in reduced row-echelon form, has no zero rows and has $m-r$ pivot columns.
</li>
</ol>
</p></div>
<div class="proof"><a knowl="./knowls/proof.PEEF.knowl">Proof</a></div>
</div>
<p>Notice that in the case where $A$ is a nonsingular matrix we know that the reduced row-echelon form of $A$ is the identity matrix (<a class="knowl" acro="NMRRI" type="Theorem" title="Nonsingular Matrices Row Reduce to the Identity matrix" knowl="./knowls/theorem.NMRRI.knowl">Theorem NMRRI</a>), so $B=I_n$.  Then the second conclusion above says $JA=B=I_n$, so $J$ is the inverse of $A$.  Thus this theorem generalizes <a class="knowl" acro="CINM" type="Theorem" title="Computing the Inverse of a Nonsingular Matrix" knowl="./knowls/theorem.CINM.knowl">Theorem CINM</a>, though the result is a “left-inverse” of $A$ rather than a “right-inverse.”</p>
<p>The third conclusion of <a class="knowl" acro="PEEF" type="Theorem" title="Properties of Extended Echelon Form" knowl="./knowls/theorem.PEEF.knowl">Theorem PEEF</a> is the most telling.  It says that $\vect{x}$ is a solution to the linear system $\linearsystem{A}{\vect{y}}$ if and only if $\vect{x}$ is a solution to the linear system $\linearsystem{B}{J\vect{y}}$.  Or said differently, if we row-reduce the augmented matrix $\augmented{A}{\vect{y}}$ we will get the augmented matrix $\augmented{B}{J\vect{y}}$.  The matrix $J$ tracks the cumulative effect of the row operations that converts $A$ to reduced row-echelon form, here effectively applying them to the vector of constants in a system of equations having $A$ as a coefficient matrix.  When $A$ row-reduces to a matrix with zero rows, then $J\vect{y}$ should also have zero entries in the same rows if the system is to be consistent.</p>
</div><div class="context"><a href="http://linear.pugetsound.edu/html/section-FS.html#subsection-EEF" class="context" title="Section FS">(in context)</a></div>
