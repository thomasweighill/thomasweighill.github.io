<div class="example" id="example-RSC5" acro="RSC5" titletext="Reducing a span in $\complex{5}$">
<h5 class="example">
<span class="type">Example</span> <span class="acro">RSC5</span> <span class="titletext">Reducing a span in $\complex{5}$</span>
</h5>
<p>Consider the set of $n=4$ vectors from $\complex{5}$,
\begin{equation*}
R=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}
=
\set{
\colvector{1\\2\\-1\\3\\2},\,
\colvector{2\\1\\3\\1\\2},\,
\colvector{0\\-7\\6\\-11\\-2},\,
\colvector{4\\1\\2\\1\\6}
}\\
\end{equation*}
and define $V=\spn{R}$.</p>
<p>To employ <a class="knowl" acro="LIVHS" type="Theorem" title="Linearly Independent Vectors and Homogeneous Systems" knowl="./knowls/theorem.LIVHS.knowl">Theorem LIVHS</a>, we form a $5\times 4$ matrix, $D$, and row-reduce to understand solutions to the homogeneous system $\homosystem{D}$,
\begin{equation*}
D=
\begin{bmatrix}
1&amp;2&amp;0&amp;4\\
2&amp;1&amp;-7&amp;1\\
-1&amp;3&amp;6&amp;2\\
3&amp;1&amp;-11&amp;1\\
2&amp;2&amp;-2&amp;6
\end{bmatrix}
\rref
\begin{bmatrix}
\leading{1}&amp;0&amp;0&amp;4\\
0&amp;\leading{1}&amp;0&amp;0\\
0&amp;0&amp;\leading{1}&amp;1\\
0&amp;0&amp;0&amp;0\\
0&amp;0&amp;0&amp;0
\end{bmatrix}
\end{equation*}
</p>
<p>We can find infinitely many solutions to this system, most of them nontrivial, and we choose any one we like to build a relation of linear dependence on $R$.   Let us begin with $x_4=1$, to find the solution
\begin{equation*}
\colvector{-4\\0\\-1\\1}
\end{equation*}
</p>
<p>So we can write the relation of linear dependence,
\begin{equation*}
(-4)\vect{v}_1+0\vect{v}_2+(-1)\vect{v}_3+1\vect{v}_4=\zerovector
\end{equation*}
</p>
<p><a class="knowl" acro="DLDS" type="Theorem" title="Dependency in Linearly Dependent Sets" knowl="./knowls/theorem.DLDS.knowl">Theorem DLDS</a> guarantees that we can solve this relation of linear dependence for <em>some</em> vector in $R$, but the choice of which one is up to us.  Notice however that $\vect{v}_2$ has a zero coefficient.  In this case, we cannot choose to solve for $\vect{v}_2$.  Maybe some other relation of linear dependence would produce a nonzero coefficient for $\vect{v}_2$ if we just had to solve for this vector.  Unfortunately, this example has been engineered to <em>always</em> produce a zero coefficient here, as you can see from solving the homogeneous system.  Every solution has $x_2=0$!</p>
<p>OK, if we are convinced that we cannot solve for $\vect{v}_2$, let us instead solve for $\vect{v}_3$,
\begin{equation*}
\vect{v}_3=(-4)\vect{v}_1+0\vect{v}_2+1\vect{v}_4=(-4)\vect{v}_1+1\vect{v}_4
\end{equation*}
</p>
<p>We now claim that this particular equation will allow us to write
\begin{equation*}
V=\spn{R}=
\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_3,\,\vect{v}_4}}=
\spn{\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_4}}
\end{equation*}
in essence declaring $\vect{v}_3$ as surplus for the task of building $V$ as a span.  This claim is an equality of two sets, so we will use <a class="knowl" acro="SE" type="Definition" title="Set Equality" knowl="./knowls/definition.SE.knowl">Definition SE</a> to establish it carefully.  Let $R^\prime=\set{\vect{v}_1,\,\vect{v}_2,\,\vect{v}_4}$ and $V^\prime=\spn{R^\prime}$.  We want to show that $V=V^\prime$.</p>
<p>First show that $V^\prime\subseteq V$.  Since every vector of $R^\prime$ is in $R$, any vector we can construct in $V^\prime$ as a linear combination of vectors from $R^\prime$ can also be constructed as a vector in $V$ by the same linear combination of the same vectors in $R$.  That was easy, now turn it around.</p>
<p>Next show that $V\subseteq V^\prime$.  Choose any $\vect{v}$ from $V$.  So there are scalars $\alpha_1,\,\alpha_2,\,\alpha_3,\,\alpha_4$ such that
\begin{align*}
\vect{v}&amp;=
\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+\alpha_3\vect{v}_3+\alpha_4\vect{v}_4\\
&amp;=\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+
\alpha_3\left((-4)\vect{v}_1+1\vect{v}_4\right)+
\alpha_4\vect{v}_4\\
&amp;=\alpha_1\vect{v}_1+\alpha_2\vect{v}_2+
\left((-4\alpha_3)\vect{v}_1+\alpha_3\vect{v}_4\right)+
\alpha_4\vect{v}_4\\
&amp;=\left(\alpha_1-4\alpha_3\right)\vect{v}_1+
\alpha_2\vect{v}_2+
\left(\alpha_3+\alpha_4\right)\vect{v}_4.
\end{align*}

</p>
<p>This equation says that $\vect{v}$ can then be written as a linear combination of the vectors in $R^\prime$ and hence qualifies for membership in $V^\prime$.  So $V\subseteq V^\prime$ and we have established that $V=V^\prime$.</p>
<p>If $R^\prime$ was also linearly dependent (it is not), we could reduce the set even further.  Notice that we could have chosen to eliminate any one of $\vect{v}_1$, $\vect{v}_3$ or $\vect{v}_4$, but somehow $\vect{v}_2$ is essential to the creation of $V$ since it cannot be replaced by any linear combination of $\vect{v}_1$, $\vect{v}_3$ or $\vect{v}_4$.</p>
<div class="context"><a href="section-LDS.html#example-RSC5" class="context" title="Section LDS">(in
context)</a></div>
</div>
