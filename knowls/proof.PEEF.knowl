<div class="proof" id="proof-PEEF" acro="PEEF" titletext="">
<h5 class="proof">
<span class="type">Proof of Theorem</span> <span class="acro">PEEF</span>
</h5>
<p>$J$ is the result of applying a sequence of row operations to $I_m$, and therefore $J$ and $I_m$ are row-equivalent.  $\homosystem{I_m}$ has only the zero solution, since $I_m$ is nonsingular (<a class="knowl" acro="NMRRI" type="Theorem" title="Nonsingular Matrices Row Reduce to the Identity matrix" knowl="./knowls/theorem.NMRRI.knowl">Theorem NMRRI</a>).  Thus, $\homosystem{J}$ also has only the zero solution (<a class="knowl" acro="REMES" type="Theorem" title="Row-Equivalent Matrices represent Equivalent Systems" knowl="./knowls/theorem.REMES.knowl">Theorem REMES</a>, <a class="knowl" acro="ESYS" type="Definition" title="Equivalent Systems" knowl="./knowls/definition.ESYS.knowl">Definition ESYS</a>) and $J$ is therefore nonsingular (<a class="knowl" acro="NSM" type="Definition" title="Null Space of a Matrix" knowl="./knowls/definition.NSM.knowl">Definition NSM</a>).</p>

<p>To prove the second part of this conclusion, first convince yourself that row operations and the matrix-vector product are associative operations.  By this we mean the following.
Suppose that $F$ is an $m\times n$ matrix that is row-equivalent to the matrix $G$.  Apply to the column vector $F\vect{w}$ the same sequence of row operations that converts $F$ to $G$.  Then the result is $G\vect{w}$.  So we can do row operations on the matrix, then do a matrix-vector product, <em>or</em> do a matrix-vector product and then do row operations on a column vector, and the result will be the same either way.  Since matrix multiplication is defined by a collection of matrix-vector products (<a class="knowl" acro="MM" type="Definition" title="Matrix Multiplication" knowl="./knowls/definition.MM.knowl">Definition MM</a>), the matrix product $FH$ will become $GH$ if we apply the same sequence of row operations to $FH$ that convert $F$ to $G$.  (This argument can be made more rigorous using elementary matrices from the upcoming <a class="knowl" acro="DM.EM" type="Subsection" title="" knowl="./knowls/subsection.DM.EM.knowl">Subsection DM.EM</a> and the associative property of matrix multiplication established in <a class="knowl" acro="MMA" type="Theorem" title="Matrix Multiplication is Associative" knowl="./knowls/theorem.MMA.knowl">Theorem MMA</a>.)  Now apply these observations to $A$.</p>

<p>Write $AI_n=I_mA$ and apply the row operations that convert $M$ to $N$.  $A$ is converted to $B$, while $I_m$ is converted to $J$, so we have $BI_n=JA$.  Simplifying the left side gives the desired conclusion.</p>

<p>For the third conclusion, we now establish the two equivalences
\begin{align*}
A\vect{x}&amp;=\vect{y} &amp;
&amp;\iff &amp;
JA\vect{x}&amp;=J\vect{y} &amp;
&amp;\iff &amp;
B\vect{x}&amp;=J\vect{y}
\end{align*}

</p>

<p>The forward direction of the first equivalence is accomplished by multiplying both sides of the matrix equality by $J$, while the backward direction is accomplished by multiplying by the inverse of $J$ (which we know exists by <a class="knowl" acro="NI" type="Theorem" title="Nonsingularity is Invertibility" knowl="./knowls/theorem.NI.knowl">Theorem NI</a> since $J$ is nonsingular).  The second equivalence is obtained simply by the substitutions given by $JA=B$.</p>

<p>The first $r$ rows of $N$ are in reduced row-echelon form, since any contiguous collection of rows taken from a matrix in reduced row-echelon form will form a matrix that is again in reduced row-echelon form (<a knowl="./knowls/exercise.RREF.T12.knowl">Exercise RREF.T12</a>).   Since the matrix $C$ is formed by removing the last $n$ entries of each these rows, the remainder is still in reduced row-echelon form.  By its construction, $C$ has no zero rows. $C$ has $r$ rows and each contains a leading 1, so there are $r$ pivot columns in $C$.</p>

<p>The final $m-r$ rows of $N$ are in reduced row-echelon form, since any contiguous collection of rows taken from a matrix in reduced row-echelon form will form a matrix that is again in reduced row-echelon form.  Since the matrix $L$ is formed by removing the first $n$ entries of each these rows, and these entries are all zero (they form the zero rows of $B$), the remainder is still in reduced row-echelon form.  $L$ is the final $m-r$ rows of the nonsingular matrix $J$, so none of these rows can be totally zero, or $J$ would not row-reduce to the identity matrix.  $L$ has $m-r$ rows and each contains a leading 1, so there are $m-r$ pivot columns in $L$.</p>

</div>
