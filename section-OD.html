<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/x-mathjax-config">

MathJax.Hub.Config({
  extensions: ["tex2jax.js",
               "TeX/AMSmath.js",
               "TeX/AMSsymbols.js",
               "http://aimath.org/mathbook/mathjaxknowl.js",
               ],
  tex2jax: {
    inlineMath: [['$','$'],["\\(","\\)"]],
    processEscapes: true,
  },
TeX: {
   Macros: {
denote: ["",4],
null: "",
sb: "_",
leading: ["\\fbox{#1}",1],
rref: "\\xrightarrow{\\text{RREF}}",
rowopswap: ["R_{#1} \\leftrightarrow R_{#2}",2],
rowopmult: ["#1 R_{#2}",2],
rowopadd: ["#1 R_{#2} + R_{#3}",3],
compose: ["#1 \\circ #2",2],
inverse: ["#1^{-1}",1],
setcomplement: ["\\overline{#1}",1],
ltinverse: ["#1^{-1}",1],
preimage: ["#1^{-1}\\left(#2\\right)",2],
lt: ["#1\\left(#2\\right)",2],
transpose: ["#1^{t}",1],
adjoint: ["#1^{\\ast}",1],
adj: ["\\transpose{\\left(\\conjugate{#1}\\right)}",1],
similar: ["\\inverse{#2}#1#2",2],
detname: ["\\det\\left(#1\\right)",1],
dimension: ["\\dim\\left(#1\\right)",1],
colvector: ["\\begin{bmatrix}#1\\end{bmatrix}",1],
vslt: ["{\\mathcal LT}\\left(#1,#2\\right)",2],
lns: ["{\\mathcal L}\\left(#1\\right)",1],
nsp: ["{\\mathcal N}\\left(#1\\right)",1],
csp: ["{\\mathcal C}\\left(#1\\right)",1],
rsp: ["{\\mathcal R}\\left(#1\\right)",1],
rng: ["{\\mathcal R}\\left(#1\\right)",1],
krn: ["{\\mathcal K}\\left(#1\\right)",1],
vectrepname: ["\\rho_{#1}",1],
vectrep: ["\\lt{\\vectrepname{#1}}{#2}",2],
vectrepinvname: ["\\ltinverse{\\vectrepname{#1}}",1],
vectrepinv: ["\\lt{\\ltinverse{\\vectrepname{#1}}}{#2}",2],
matrixrepcolumns: ["\\left\\lbrack \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{1}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{2}}}\\right| \\left.\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{3}}}\\right| \\ldots \\left|\\vectrep{#2}{\\lt{#1}{\\vect{#3}\\sb{#4}}}\\right.  \\right\\rbrack",4],
matrixrep: ["M^{#1}_{#2,#3}",3],
cbm: ["C_{#1,#2}",2],
jordan: ["J_{#1}\\left(#2\\right)",2],
restrict: ["{#1}|\\sb{#2}",2],
indx: ["\\iota_{#1}|\\left(#2\\right)",2],
rank: ["r\\left(#1\\right)",1],
nullity: ["n\\left(#1\\right)",1],
eigenspace: ["{\\mathcal E}_{#1}\\left(#2\\right)",2],
eigensystem: ["\\lambda&=#2&\\eigenspace{#1}{#2}&=\\spn{\\set{#3}}",3],
geneigenspace: ["{\\mathcal G}_{#1}\\left(#2\\right)",2],
innerproduct: ["\\FCLAlangle#1,#2\\FCLArangle",2],
spn: ["\\FCLAlangle#1\\FCLArangle",1],
card: ["\\FCLAlvert#1\\FCLArvert",1],
detbars: ["\\FCLAlvert#1\\FCLArvert",1],
modulus: ["\\FCLAlvert#1\\FCLArvert",1],
norm: ["\\FCLAlVert#1\\FCLArVert",1],
matrixentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
vectorentry: ["\\FCLAlbrack#1\\FCLArbrack_{#2}",2],
matrixcolumns: ["\\left\\lbrack\\vect{#1}_{1}|\\vect{#1}_{2}|\\vect{#1}_{3}|\\ldots|\\vect{#1}_{#2}\\right\\rbrack",2],
vectorlist: ["\\vect{#1}_{1}, \\vect{#1}_{2}, \\vect{#1}_{3}, \\ldots, \\vect{#1}_{#2}",2],
scalarlist: ["{#1}_{1}, {#1}_{2}, {#1}_{3}, \\ldots, {#1}_{#2}",2],
set: ["\\FCLAlsetbrack#1\\FCLArsetbrack",1],
setparts: ["\\FCLAlsetbrack\\FCLAlnull #1 \\FCLArvert #2 \\FCLArsetbrack",2],
algmult: ["\\alpha_{#1}\\left(#2\\right)",2],
geomult: ["\\gamma_{#1}\\left(#2\\right)",2],
charpoly: ["p_{#1}\\left(#2\\right)",2],
augmented: ["\\FCLAlbrack\\FCLAlnull#1\\FCLArvert#2\\FCLArbrack",2],
linearsystem: ["{\\mathcal L}{\\mathcal S}\\left(#1,#2\\right)",2],
homosystem: ["\\linearsystem{#1}{\\zerovector}",1],
lincombo: ["#1_{1}\\vect{#2}_{1}+#1_{2}\\vect{#2}_{2}+#1_{3}\\vect{#2}_{3}+\\cdots +#1_{#3}\\vect{#2}_{#3}",3],
submatrix: ["#1\\left(#2|#3\\right)",3],
elemswap: ["E_{#1,#2}",2],
elemmult: ["E_{#2}\\left(#1\\right)",2],
elemadd: ["E_{#2,#3}\\left(#1\\right)",3],
ltdefn: ["#1 : #2 \\rightarrow #3",3],
zerovector: "\\vect{0}",
zeromatrix: "{\\mathcal 0}",
vect: ["{\\bf #1}",1],
conjugate: ["\\overline{#1}",1],
ds: "\\oplus",
isomorphic: "\\cong",
complexes: "{\\mathbb C}",
complex: ["{\\mathbb C}^{#1}",1],
real: ["{\\mathbb R}^{#1}",1],
FCLAlangle: "\\left\\langle",
FCLArangle: "\\right\\rangle",
FCLAlbrack: "\\left\\lbrack",
FCLArbrack: "\\right\\rbrack",
FCLAlvert: "\\left\\lvert",
FCLArvert: "\\right\\rvert",
FCLAlVert: "\\left\\lVert",
FCLArVert: "\\right\\rVert",
FCLAlnull: "\\left.",
FCLArnull: "\\right.",
FCLAlsetbrack: "\\left\\{",
FCLArsetbrack: "\\right\\}",
intertext: ["\\\\ \\text{#1}\\\\ ",1],
}
},
  CommonHTML: { scale: 85 },
  menuSettings: { zscale: "150%", zoom: "Double-Click" }
});

</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full"></script><script type="text/javascript" src="http://sagecell.sagemath.org/static/jquery.min.js"></script><script src="http://sagecell.sagemath.org/embedded_sagecell.js"></script><style type="text/css">
.sagecell .CodeMirror-scroll {
    overflow-y: hidden;
    overflow-x: auto;
}
.sagecell .CodeMirror {
    height: auto;
}


.sagecell-practice .CodeMirror-scroll {
  height: 100px;
}

.sagecell button.sagecell_evalButton {
    font-size: 80%;
}

.sagecell_sessionContainer {
    margin-bottom:1em;
}
</style>
<link href="http://aimath.org/knowlstyle.css" rel="stylesheet" type="text/css">
<script type="text/javascript" src="http://aimath.org/knowl.js"></script><link href="http://fonts.googleapis.com/css?family=Istok+Web:400,400italic,700%7CSource+Code+Pro:400" rel="stylesheet" type="text/css">
<link href="css/beezer.css" rel="stylesheet" type="text/css">
<link href="css/beezer-addon.css" rel="stylesheet" type="text/css">
</head>
<body>
<div id="header">
<div id="logo"><a href="http://linear.pugetsound.edu/"><img src="images/cover-84x120.png"></a></div>
<div class="right"><div class="bread">
<a href="fcla.html">A First Course in Linear Algebra</a> » <a href="chapter-R.html">Representations</a> » <a href="section-OD.html">Orthonormal Diagonalization</a> »</div></div>
<div id="title"><span id="title-content">Orthonormal Diagonalization</span></div>
</div>
<div id="sidebar">
<h2 class="link"><a href="fcla.html">A First Course in Linear Algebra</a></h2>
<ul class="list">
<li><a href="preface.html">Preface</a></li>
<li><a href="acknowledgements.html">Dedication and Acknowledgements</a></li>
</ul>
<h2 class="link"><a href="chapter-SLE.html">Systems of Linear Equations</a></h2>
<ul class="list">
<li><a href="section-WILA.html">What is Linear Algebra?</a></li>
<li><a href="section-SSLE.html">Solving Systems of Linear Equations</a></li>
<li><a href="section-RREF.html">Reduced Row-Echelon Form</a></li>
<li><a href="section-TSS.html">Types of Solution Sets</a></li>
<li><a href="section-HSE.html">Homogeneous Systems of Equations</a></li>
<li><a href="section-NM.html">Nonsingular Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-V.html">Vectors</a></h2>
<ul class="list">
<li><a href="section-VO.html">Vector Operations</a></li>
<li><a href="section-LC.html">Linear Combinations</a></li>
<li><a href="section-SS.html">Spanning Sets</a></li>
<li><a href="section-LI.html">Linear Independence</a></li>
<li><a href="section-LDS.html">Linear Dependence and Spans</a></li>
<li><a href="section-O.html">Orthogonality</a></li>
</ul>
<h2 class="link"><a href="chapter-M.html">Matrices</a></h2>
<ul class="list">
<li><a href="section-MO.html">Matrix Operations</a></li>
<li><a href="section-MM.html">Matrix Multiplication</a></li>
<li><a href="section-MISLE.html">Matrix Inverses and Systems of Linear Equations</a></li>
<li><a href="section-MINM.html">Matrix Inverses and Nonsingular Matrices</a></li>
<li><a href="section-CRS.html">Column and Row Spaces</a></li>
<li><a href="section-FS.html">Four Subsets</a></li>
</ul>
<h2 class="link"><a href="chapter-VS.html">Vector Spaces</a></h2>
<ul class="list">
<li><a href="section-VS.html">Vector Spaces</a></li>
<li><a href="section-S.html">Subspaces</a></li>
<li><a href="section-LISS.html">Linear Independence and Spanning Sets</a></li>
<li><a href="section-B.html">Bases</a></li>
<li><a href="section-D.html">Dimension</a></li>
<li><a href="section-PD.html">Properties of Dimension</a></li>
</ul>
<h2 class="link"><a href="chapter-D.html">Determinants</a></h2>
<ul class="list">
<li><a href="section-DM.html">Determinant of a Matrix</a></li>
<li><a href="section-PDM.html">Properties of Determinants of Matrices</a></li>
</ul>
<h2 class="link"><a href="chapter-E.html">Eigenvalues</a></h2>
<ul class="list">
<li><a href="section-EE.html">Eigenvalues and Eigenvectors</a></li>
<li><a href="section-PEE.html">Properties of Eigenvalues and Eigenvectors</a></li>
<li><a href="section-SD.html">Similarity and Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-LT.html">Linear Transformations</a></h2>
<ul class="list">
<li><a href="section-LT.html">Linear Transformations</a></li>
<li><a href="section-ILT.html">Injective Linear Transformations</a></li>
<li><a href="section-SLT.html">Surjective Linear Transformations</a></li>
<li><a href="section-IVLT.html">Invertible Linear Transformations</a></li>
</ul>
<h2 class="link"><a href="chapter-R.html">Representations</a></h2>
<ul class="list">
<li><a href="section-VR.html">Vector Representations</a></li>
<li><a href="section-MR.html">Matrix Representations</a></li>
<li><a href="section-CB.html">Change of Basis</a></li>
<li><a href="section-OD.html">Orthonormal Diagonalization</a></li>
</ul>
<h2 class="link"><a href="chapter-P.html">Preliminaries</a></h2>
<ul class="list">
<li><a href="section-CNO.html">Complex Number Operations</a></li>
<li><a href="section-SET.html">Sets</a></li>
</ul>
<h2 class="link"><a href="archetypes.html">Archetypes</a></h2>
<ul class="list">
<li> 
            <a href="archetype-A.html">A</a><a href="archetype-B.html">B</a><a href="archetype-C.html">C</a><a href="archetype-D.html">D</a><a href="archetype-E.html">E</a><a href="archetype-F.html">F</a><a href="archetype-G.html">G</a><a href="archetype-H.html">H</a><a href="archetype-I.html">I</a><a href="archetype-J.html">J</a><a href="archetype-K.html">K</a><a href="archetype-L.html">L</a><a href="archetype-M.html">M</a>
</li>
<li> 
            <a href="archetype-N.html">N</a><a href="archetype-O.html">O</a><a href="archetype-P.html">P</a><a href="archetype-Q.html">Q</a><a href="archetype-R.html">R</a><a href="archetype-S.html">S</a><a href="archetype-T.html">T</a><a href="archetype-U.html">U</a><a href="archetype-V.html">V</a><a href="archetype-W.html">W</a><a href="archetype-X.html">X</a>
</li>
</ul>
<h2 class="link"><a href="reference.html">Reference</a></h2>
<ul class="list">
<li><a href="notation.html">Notation</a></li>
<li><a href="definitions.html">Definitions</a></li>
<li><a href="theorems.html">Theorems</a></li>
<li><a href="diagrams.html">Diagrams</a></li>
<li><a href="examples.html">Examples</a></li>
<li><a href="sage.html">Sage</a></li>
<li><a href="techniques.html">Proof Techniques</a></li>
<li><a href="GFDL.html">GFDL License</a></li>
</ul>
</div>
<div id="main"><div id="content"><div class="section" id="section-OD" acro="OD" titletext="Orthonormal Diagonalization">
<h3 class="section">
<span class="type">Section</span> <span class="acro">OD</span> <span class="titletext">Orthonormal Diagonalization</span>
</h3>
<div class="introduction">
<p>We have seen in <a href="section-SD.html" title="Similarity and Diagonalization">Section SD</a> that under the right conditions a square matrix is similar to a diagonal matrix.  We recognize now, via <a class="knowl" acro="SCB" type="Theorem" title="Similarity and Change of Basis" knowl="./knowls/theorem.SCB.knowl">Theorem SCB</a>, that a similarity transformation is a change of basis on a matrix representation.  So we can now discuss the choice of a basis used to build a matrix representation, and decide if some bases are better than others for this purpose.  This will be the tone of this section.  We will also see that every matrix has a reasonably useful matrix representation, and we will discover a new class of diagonalizable linear transformations.  First we need some basic facts about triangular matrices.</p>

</div>
<div class="subsection" id="subsection-TM" acro="TM" titletext="Triangular Matrices">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">TM</span> <span class="titletext">Triangular Matrices</span>
</h4>
<p>An upper, or lower, triangular matrix is exactly what it sounds like it should be, but here are the two relevant definitions.</p>
<div class="definition" id="definition-UTM" acro="UTM" titletext="Upper Triangular Matrix">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">UTM</span> <span class="titletext"> Upper Triangular Matrix</span>
</h5>
<p>The $n\times n$ square matrix $A$ is <em class="term">upper triangular</em> if $\matrixentry{A}{ij} =0$ whenever $i&gt;j$.</p>
</div>
<div class="definition" id="definition-LTM" acro="LTM" titletext="Lower Triangular Matrix">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">LTM</span> <span class="titletext"> Lower Triangular Matrix</span>
</h5>
<p>The $n\times n$ square matrix $A$ is <em class="term">lower triangular</em> if $\matrixentry{A}{ij} =0$ whenever
$i&lt;j$.</p>
</div>
<p>Obviously, properties of a lower triangular matrices will have analogues for upper triangular matrices.  Rather than stating two very similar theorems, we will say that matrices are “triangular of the same type” as a convenient shorthand to cover both possibilities and then give a proof for just one type.</p>
<div class="theorem" id="theorem-PTMT" acro="PTMT" titletext="Product of Triangular Matrices is Triangular">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">PTMT</span><span class="titletext"> Product of Triangular Matrices is Triangular</span>
</h5>
<div class="statement"><p>Suppose that $A$ and $B$ are square matrices of size $n$ that are triangular of the same type.  Then $AB$ is also triangular of that type.</p></div>
<div class="proof"><a knowl="./knowls/proof.PTMT.knowl">Proof</a></div>
</div>
<p>The inverse of a triangular matrix is triangular, of the same type.</p>
<div class="theorem" id="theorem-ITMT" acro="ITMT" titletext="Inverse of a Triangular Matrix is Triangular">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">ITMT</span><span class="titletext"> Inverse of a Triangular Matrix is Triangular</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a nonsingular matrix of size $n$ that is triangular.  Then the inverse of $A$, $\inverse{A}$, is triangular of the same type.  Furthermore, the diagonal entries of $\inverse{A}$ are the reciprocals of the corresponding diagonal entries of $A$.  More precisely, $\matrixentry{\inverse{A}}{ii}=\matrixentry{A}{ii}^{-1}$.</p></div>
<div class="proof"><a knowl="./knowls/proof.ITMT.knowl">Proof</a></div>
</div>
</div>
<div class="subsection" id="subsection-UTMR" acro="UTMR" titletext="Upper Triangular Matrix Representation">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">UTMR</span> <span class="titletext">Upper Triangular Matrix Representation</span>
</h4>
<p>Not every matrix is diagonalizable, but every linear transformation has a matrix representation that is an upper triangular matrix, and the basis that achieves this representation is especially pleasing.  Here is the theorem.</p>
<div class="theorem" id="theorem-UTMR" acro="UTMR" titletext="Upper Triangular Matrix Representation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">UTMR</span><span class="titletext"> Upper Triangular Matrix Representation</span>
</h5>
<div class="statement"><p>Suppose that $\ltdefn{T}{V}{V}$ is a linear transformation.  Then there is a basis $B$ for $V$ such that the matrix representation of $T$ relative to $B$, $\matrixrep{T}{B}{B}$, is an upper triangular matrix.  Each diagonal entry is an eigenvalue of $T$, and if $\lambda$ is an eigenvalue of $T$, then $\lambda$ occurs $\algmult{T}{\lambda}$ times on the diagonal.</p></div>
<div class="proof"><a knowl="./knowls/proof.UTMR.knowl">Proof</a></div>
</div>
<p>A key step in this proof was the construction of the subspace $W$ with dimension strictly less than that of $V$.  This required an eigenvalue/eigenvector pair, which was guaranteed to us by <a class="knowl" acro="EMHE" type="Theorem" title="Every Matrix Has an Eigenvalue" knowl="./knowls/theorem.EMHE.knowl">Theorem EMHE</a>.  Digging deeper, the proof of <a class="knowl" acro="EMHE" type="Theorem" title="Every Matrix Has an Eigenvalue" knowl="./knowls/theorem.EMHE.knowl">Theorem EMHE</a> requires that we can factor polynomials completely, into linear factors.  This will not always happen if our set of scalars is the reals, $\real{\null}$.  So this is our final explanation of our choice of the complex numbers, $\complexes$, as our set of scalars.  In $\complexes$ polynomials factor completely, so every matrix has at least one eigenvalue, and an inductive argument will get us to upper triangular matrix representations.</p>
<p>In the case of linear transformations defined on $\complex{m}$, we can use the inner product (<a class="knowl" acro="IP" type="Definition" title="Inner Product" knowl="./knowls/definition.IP.knowl">Definition IP</a>) profitably to fine-tune the basis that yields an upper triangular matrix representation.  Recall that the adjoint of matrix $A$ (<a class="knowl" acro="A" type="Definition" title="Adjoint" knowl="./knowls/definition.A.knowl">Definition A</a>) is written as $\adjoint{A}$.</p>
<div class="theorem" id="theorem-OBUTR" acro="OBUTR" titletext="Orthonormal Basis for Upper Triangular Representation">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">OBUTR</span><span class="titletext"> Orthonormal Basis for Upper Triangular Representation</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix.  Then there is a unitary matrix $U$, and an upper triangular matrix $T$, such that
\begin{align*}
\adjoint{U}AU=T
\end{align*}

and $T$ has the eigenvalues of $A$ as the entries of the diagonal.</p></div>
<div class="proof"><a knowl="./knowls/proof.OBUTR.knowl">Proof</a></div>
</div>
</div>
<div class="subsection" id="subsection-NM" acro="NM" titletext="Normal Matrices">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">NM</span> <span class="titletext">Normal Matrices</span>
</h4>
<p>Normal matrices comprise a broad class of interesting matrices, many of which we have met already.  But they are most interesting since they define exactly which matrices we can diagonalize via a unitary matrix.  This is the upcoming <a class="knowl" acro="OD" type="Theorem" title="Orthonormal Diagonalization" knowl="./knowls/theorem.OD.knowl">Theorem OD</a>.  Here is the definition.</p>
<div class="definition" id="definition-NRML" acro="NRML" titletext="Normal Matrix">
<h5 class="definition">
<span class="type">Definition </span><span class="acro">NRML</span> <span class="titletext"> Normal Matrix</span>
</h5>
<p>The square matrix $A$ is normal if $\adjoint{A}A=A\adjoint{A}$.</p>
</div>
<p>So a normal matrix commutes with its adjoint.  Part of the beauty of this definition is that it includes many other types of matrices.  A diagonal matrix will commute with its adjoint, since the adjoint is again diagonal and the entries are just conjugates of the entries of the original diagonal matrix.  A Hermitian (self-adjoint) matrix (<a class="knowl" acro="HM" type="Definition" title="Hermitian Matrix" knowl="./knowls/definition.HM.knowl">Definition HM</a>) will trivially commute with its adjoint, since the two matrices are the same.  A real, symmetric matrix is Hermitian, so these matrices are also normal.  A unitary matrix (<a class="knowl" acro="UM" type="Definition" title="Unitary Matrices" knowl="./knowls/definition.UM.knowl">Definition UM</a>) has its adjoint as its inverse, and inverses commute (<a class="knowl" acro="OSIS" type="Theorem" title="One-Sided Inverse is Sufficient" knowl="./knowls/theorem.OSIS.knowl">Theorem OSIS</a>), so unitary matrices are normal.  Another class of normal matrices is the skew-symmetric matrices.  However, these broad descriptions still do not capture all of the normal matrices, as the next example shows.</p>
<div class="example" id="example-ANM" acro="ANM" titletext="A normal matrix"><h5 class="example">
<a knowl="./knowls/example.ANM.knowl"><span class="type">Example</span> <span class="acro">ANM</span></a> <span class="titletext">A normal matrix</span>
</h5></div>
</div>
<div class="subsection" id="subsection-OD" acro="OD" titletext="Orthonormal Diagonalization">
<h4 class="subsection">
<span class="type">Subsection</span> <span class="acro">OD</span> <span class="titletext">Orthonormal Diagonalization</span>
</h4>
<p>A diagonal matrix is very easy to work with in matrix multiplication (<a class="knowl" acro="HPDM" type="Example" title="High power of a diagonalizable matrix" knowl="./knowls/example.HPDM.knowl">Example HPDM</a>) and an orthonormal basis also has many advantages (<a class="knowl" acro="COB" type="Theorem" title="Coordinates and Orthonormal Bases" knowl="./knowls/theorem.COB.knowl">Theorem COB</a>).  How about converting a matrix to a diagonal matrix through a similarity transformation using a unitary matrix (i.e.  build a diagonal matrix representation with an orthonormal matrix)?  That'd be fantastic!  When can we do this?  We can always accomplish this feat when the matrix is normal, and normal matrices are the only ones that behave this way.  Here is the theorem.</p>
<div class="theorem" id="theorem-OD" acro="OD" titletext="Orthonormal Diagonalization">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">OD</span><span class="titletext"> Orthonormal Diagonalization</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a square matrix.   Then there is a unitary matrix $U$ and a diagonal matrix $D$, with diagonal entries equal to the eigenvalues of $A$, such that $\adjoint{U}AU=D$ if and only if $A$ is a normal matrix.</p></div>
<div class="proof"><a knowl="./knowls/proof.OD.knowl">Proof</a></div>
</div>
<p>We can rearrange the conclusion of this theorem to read $A=UD\adjoint{U}$.  Recall that a unitary matrix can be viewed as a geometry-preserving transformation (isometry), or more loosely as a rotation of sorts.  Then a matrix-vector product, $A\vect{x}$, can be viewed instead as a sequence of three transformations.  $\adjoint{U}$ is unitary, and so is a rotation.  Since $D$ is diagonal, it just multiplies each entry of a vector by a scalar.  Diagonal entries that are positive or negative, with absolute values bigger or smaller than 1 evoke descriptions like reflection, expansion and contraction.  Generally we can say that $D$ “stretches” a vector in each component.  Final multiplication by $U$ undoes (inverts) the rotation performed by $\adjoint{U}$.  So a normal matrix is a rotation-stretch-rotation transformation.</p>
<p>The orthonormal basis formed from the columns of $U$ can be viewed as a system of mutually perpendicular axes.  The rotation by $\adjoint{U}$ allows the transformation by $A$ to be replaced by the simple transformation $D$ along these axes, and then $D$ brings the result back to the original coordinate system.  For this reason <a class="knowl" acro="OD" type="Theorem" title="Orthonormal Diagonalization" knowl="./knowls/theorem.OD.knowl">Theorem OD</a> is known as the <em class="term">Principal Axis Theorem</em>.</p>
<p>The columns of the unitary matrix in <a class="knowl" acro="OD" type="Theorem" title="Orthonormal Diagonalization" knowl="./knowls/theorem.OD.knowl">Theorem OD</a> create an especially nice basis for use with the normal matrix.  We record this observation as a theorem.</p>
<div class="theorem" id="theorem-OBNM" acro="OBNM" titletext="Orthonormal Bases and Normal Matrices">
<h5 class="theorem">
<span class="type">Theorem </span><span class="acro">OBNM</span><span class="titletext"> Orthonormal Bases and Normal Matrices</span>
</h5>
<div class="statement"><p>Suppose that $A$ is a normal matrix of size $n$.  Then there is an orthonormal basis of $\complex{n}$ composed of eigenvectors of $A$.</p></div>
<div class="proof"><a knowl="./knowls/proof.OBNM.knowl">Proof</a></div>
</div>
<p>In a vague way <a class="knowl" acro="OBNM" type="Theorem" title="Orthonormal Bases and Normal Matrices" knowl="./knowls/theorem.OBNM.knowl">Theorem OBNM</a> is an improvement on <a class="knowl" acro="HMOE" type="Theorem" title="Hermitian Matrices have Orthogonal Eigenvectors" knowl="./knowls/theorem.HMOE.knowl">Theorem HMOE</a> which said that eigenvectors of a Hermitian matrix for different eigenvalues are always orthogonal.  Hermitian matrices are normal and we see that we can find at least one basis where <em>every</em> pair of eigenvectors is orthogonal.  Notice that this is not a generalization, since <a class="knowl" acro="HMOE" type="Theorem" title="Hermitian Matrices have Orthogonal Eigenvectors" knowl="./knowls/theorem.HMOE.knowl">Theorem HMOE</a> states a weak result which applies to many (but not all) pairs of eigenvectors, while <a class="knowl" acro="OBNM" type="Theorem" title="Orthonormal Bases and Normal Matrices" knowl="./knowls/theorem.OBNM.knowl">Theorem OBNM</a> is a seemingly stronger result, but only asserts that there is one collection of eigenvectors with the stronger property.</p>
<p>Given an $n\times n$ matrix $A$, an orthonormal basis for $\complex{n}$, comprised of eigenvectors of $A$ is an extremely useful basis to have at the service of the matrix $A$.  Why do we say this?  We can consider the vectors of a basis as a preferred set of directions, known as “axes,” which taken together might also be called a “coordinate system.”  The standard basis of <a class="knowl" acro="SUV" type="Definition" title="Standard Unit Vectors" knowl="./knowls/definition.SUV.knowl">Definition SUV</a> could be considered the default, or prototype, coordinate system.  When a basis is orthornormal, we can consider the directions to be standardized to have unit length, and we can consider the axes as being mutually perpendicular.  But there is more — let us be a bit more formal.</p>
<p>Suppose $U$ is a matrix whose columns are an orthonormal basis of eigenvectors of the $n\times n$ matrix $A$.  So, in particular $U$ is a unitary matrix (<a class="knowl" acro="CUMOS" type="Theorem" title="Columns of Unitary Matrices are Orthonormal Sets" knowl="./knowls/theorem.CUMOS.knowl">Theorem CUMOS</a>).  For a vector $\vect{x}\in\complex{n}$, use the notation $\hat{\vect{x}}$ for the vector representation of $\vect{x}$ relative to the orthonormal basis.  So the entries of $\hat{\vect{x}}$, used in a linear combination of the columns of $U$ will create $\vect{x}$.  With <a class="knowl" acro="MVP" type="Definition" title="Matrix-Vector Product" knowl="./knowls/definition.MVP.knowl">Definition MVP</a>, we can write this relationship as
\begin{align*}
U\hat{\vect{x}} = \vect{x}
\end{align*}

Since $\adjoint{U}$ is the inverse of $U$ (<a class="knowl" acro="UM" type="Definition" title="Unitary Matrices" knowl="./knowls/definition.UM.knowl">Definition UM</a>), we can rearrange this equation as
\begin{align*}
\hat{\vect{x}} = \adjoint{U}\vect{x}
\end{align*}

This says we can easily create the vector representation relative to the orthonormal basis with a matrix-vector product of the adjoint of $U$.  Note that the adjoint is much easier to compute than a matrix inverse, which would be one general way to obtain a vector representation.  This is our first observation about coordinatization relative to orthonormal basis.  However, we already knew this, as we just have <a class="knowl" acro="COB" type="Theorem" title="Coordinates and Orthonormal Bases" knowl="./knowls/theorem.COB.knowl">Theorem COB</a> in disguise (see <a knowl="./knowls/exercise.OD.T20.knowl">Exercise OD.T20</a>).</p>
<p>We also know that orthonormal bases play nicely with inner products.  <a class="knowl" acro="UMPIP" type="Theorem" title="Unitary Matrices Preserve Inner Products" knowl="./knowls/theorem.UMPIP.knowl">Theorem UMPIP</a> says unitary matrices preserve inner products (and hence norms).  More geometrically, lengths and angles are preserved by multiplication by a unitary matrix.  Using our notation, this becomes
\begin{align*}
\innerproduct{\vect{x}}{\vect{y}}
=\innerproduct{U\hat{\vect{x}}}{U\hat{\vect{y}}}
=\innerproduct{\hat{\vect{x}}}{\hat{\vect{y}}}
\end{align*}

So we can compute inner products with the original vectors, or with their representations, and obtain the same result.  It follows that norms, lengths, and angles can all be computed with the original vectors or with the representations in the new coordinate system based on an orthonormal basis.</p>
<p>So far we have not really said anything new, nor has the matrix $A$, or its eigenvectors, come into play.  We know that a matrix is really a linear transformation, so we express this view of a matrix as a function by writing generically that $A\vect{x}=\vect{y}$.  The matrix $U$ will diagonalize $A$, creating the diagonal matrix $D$ with diagonal entries equal to the eigenvalues of $A$.  We can write this as $\adjoint{U}AU = D$ and convert to $\adjoint{U}A=D\adjoint{U}$.  Then we have
\begin{align*}
\hat{\vect{y}}
=\adjoint{U}\vect{y}
=\adjoint{U}A\vect{x}
=D\adjoint{U}\vect{x}
=D\hat{\vect{x}}
\end{align*}

So with the coordinatized vectors, the transformation by the matrix $A$ can be accomplished with multiplication by a diagonal matrix $D$.  A moment's thought should convince you that a matrix-vector product with a diagonal matrix is exeedingly simple computationally.  Geometrically, this is simply stretching, contracting and/or reflecting in the direction of each basis vector (“axis”).  And the multiples used for these changes are the diagonal entries of the diagonal matrix, the eigenvalues of $A$.</p>
<p>So the new coordinate system (provided by the orthonormal basis of eigenvectors) is a collection of mutually perpendicular unit vectors where inner products are preserved, and the action of the matrix $A$ is described by multiples (eigenvalues) of the entries of the coordinatized versions of the vectors.  Nice.</p>
</div>
<div class="readingquestions" id="readingquestions-OD" titletext="Reading Questions"><h4 class="readingquestions"><a knowl="./knowls/reading.OD.knowl"><span class="titletext">Reading Questions</span></a></h4></div>
<div class="exercisesubsection" id="exercisesubsection-OD" titletext="Exercises"><h4 class="exercises"><a knowl="./knowls/exercises.OD.knowl"><span class="titletext">Exercises</span></a></h4></div>
</div></div></div>
<!-- Google Analytics Code -->
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script><script type="text/javascript">
var pageTracker = _gat._getTracker("UA-6386040-1");
pageTracker._trackPageview();
</script>
<!-- End: Google Analytics Code -->
<!-- StatCounter Code -->
<script type="text/javascript">
var sc_project=8375157;
var sc_invisible=1;
var sc_security="c03f6ece";
</script><script type="text/javascript" src="http://www.statcounter.com/counter/counter.js"></script><noscript><div class="statcounter"><a title="web analytics" href="http://statcounter.com/" target="_blank"><img class="statcounter" src="http://c.statcounter.com/8375157/0/c03f6ece/1/" alt="web analytics"></a></div></noscript>
<!-- End: StatCounter Code -->
</body>
</html>
